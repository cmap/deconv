# Results

The contest attracted 294 participants who made 820 code submissions overall with a variety of different analysis methods (see \nameref{table-1} for an overview of the best methods). Here we report the top-four performing methods based on the score obtained on the holdout data. 

The winning solution (by a competitor from the United States with a degree in Physics from the University of Kansas) was a random forest algorithm (combining 10 different trees). Each tree was trained using 60 different features of the data; 50 features were from the distribution of measurements within the gene pair, the remaining 10 features were measures of the association between the distribution of values within the pair and aggregate values at the plate and experiment level.

The second solution (by a competitor from Poland with a Master's degree in Computer Science from The Lodz University of Technology) used the Expectation-Maximization (EM) method for an initial clustering of the observed values, assuming the distribution was a mixture of two log-normal distributions (Gaussian mixture model). It then fit a plate-wide distribution of the estimated expressions and cluster sizes for each gene, using the results to adjust the assignment of peaks to the low- and high-proportion genes. 
 
The third solution (by a competitor from India with a bachelor's degree in Computer Science) was a  basic k-means algorithm with random initial points to avoid local minima along with other fine adjustments to minimize the impact of extreme outliers and improve speed.
  
The fourth solution (by a competitor from Ukraine with a bachelor's degree in Computer Science from the Cherkasy National University) was a Convolutional Neural Network (CNN). The algorithm first filtered the data to remove extreme outlier measurements, transforming the filtered data into a 32-bin histogram for each pair of genes. The CNN consisted of two parts. The first part used the classic U-net architecture [@ronneberger2015u] (a contracting path to capture context and a symmetric expanding path) to provide adequate representation of the data. The output of this network was then used to label bins into one of the two genes for each pair, and to predict the exact value within the bin. This second step was achieved using two subnetworks with the same architecture but weights trained separately (using a mean squared error loss function).

## Clustering by method and perturbagen type 

Given the variety of methods represented amongst the prize-winning solutions, we sought to assess whether there were notable differences in the predictions generated. Using the holdout dataset, we generated the contestants' deconvolution data (DECONV) and the corresponding differential expression (DE) values. We obtained the DE values by using the robust z-score procedure by @subramanian2017next. We then compared the results using a two-dimensional t-SNE projection [@maaten2008visualizing]. 

For the most part, the DECONV data clustered by pertubagen type, except for the ground truth UNI data, which appears distinct from the deconvolution samples (\Fig{figure-3}, A and B). 
Separating the samples by method revealed commonalities in the predictions generated by similar approaches (\Fig{figure-3}, C). For example, the decision tree regressor (DTR) algorithms have similar 'footprints' in the projection, as do the k-means and Gaussian mixture model (GMM) algorithms. This suggests that in general similar algorithms generate predictions with similar properties. However, after the transformation to DE data, the t-SNE projection appears more homogenous (\Fig{figure-3} D), indicating that perturbagen-type and algorithm-specific effects have greatly reduced. The reduced clustering at the DE level is reassuring given the downstream analysis is often focused on DE values.

## Correlation accuracy

We tested the accuracy of the solutions by comparing the distribution of the genewise Spearman's rank correlation ($\rho$) between the UNI data and the values obtained by the competitors through the deconvolution of DUO data. We did so for the shRNA and compound perturbagens separately and, within each perturbagen type, we compared the results for the subsets of 488 genes in high and low bead proportion. 

We found that the winner's cumulative distribution of $\rho$'s was shifted to the right of the benchmark's (**FIGURE REF**) for both perturbagen types and gene subsets, with a notably larger shift for the genes in low bead proportions. This result indicated considerable improvements in accuracy. On average, the winner's improvement over the benchmark was of 5 percentage points (with standard error of half a point) for the genes in low bead proportion and of 2 percentage points (with standard error of half a point) for the genes in high bead proportions. 

The analysis of the top-nine performing algorithms showed similar results (**FIGURE REF**): the average Spearman's rank correlation was generally greater for genes in high bead proportions (relative to low bead proportions) and for genes in shRNA experiments (relative to compounds). The k-means benchmark performed consistently worse than all the other methods with respect to the genes in low bead proportions, although it performed well in the high-proportion genes. 

These results showed that the winning method outperformed on average the other approaches in nearly all comparisons. So, we wanted to test to what extent the winning algorithm was the best predictor for a given gene relative to the others. We ranked the top-nine algorithms by the average correlation metric for each of the 976 landmark genes and selected the best algorithm for each gene across all the experiments. The winner was the best performer for about a third (30%) of the genes. The second-placed gaussian mixture model was the best performer for about one fourth (20%). Thus, the top two submissions achieved the highest correlation in more than half of the genes in our sample. Even so, all but a few algorithms were the best performers for at least 5% of the genes, suggesting some complementarity between these algorithms.

## Detection of extreme modulations

Downstream analysis of gene expression profiles is often centered on the detection of genes notably up- or down-regulated by perturbation, or extreme modulations. These genes exhibit exceedingly high (or low) DE values (above a given threshold). Using the UNI DE values as the ground truth, we evaluated the detection accuracy of each solution by computing the corresponding area under the curve (AUC) for each perturbagen type. 

All the solutions achieved a good average detection accuracy of extreme modulations (AUC > .87 and AUC > .91 for shRNA and compounds respectively) with improvements over the benchmark in both the shRNA and compound perturbagen types (**FIGURE REF**). 

For a subset of data, we further used targeted gene knockdown (KD) experiments as the ground truth. These are experiments involving an shRNA targeting one of the 976 landmark genes, which should exhibit a very low DE value (highly negative).For each such experiment, we defined a successful KD as one in which the DE value is less than -2 and the gene-wise rank is less than 10, meaning that the targeted gene achieves one of its lowest DE values in the experiment where it was targeted. We computed the KD success frequency, or recall, for each competitor algorithm and the benchmark. We also computed the recall for the UNI data yielding an estimate of the maximum achievable recall, which in this case was $0.8$. 

We observed that nearly all algorithms achieved both a higher recall and a higher precision than the benchmark solution (**FIGURE REF**). These results suggest that the algorithm improvements translate to improvements in biologically relevant metrics used in common applications of L1000 data.

## Reduced variation across replicate samples

Our data contain between 4 to 11 replicate samples for each perturbagen. Keeping low the inter-replicate variation is of crucial practical importance to biologists because it means higher reproducibility of results. With this goal in mind, the benchmark k-means solution is likely suboptimal because it does little to mitigate the discrepancy in variability between the genes measured with high and low bead proportions. 

We compared the inter-replicate variance of the predictions made by each method for genes in low and high bead proportions. We found significant improvements in the inter-replicate variance of the competitors over the benchmark with the difference being more visible for genes in low bead proportions (**FIGURE REF**). Among the competitors, the winning method, which was the most accurate on average, was also the most efficient achieving the lowest inter-replicate variance.

## Computational speed

The speed improvements over the benchmark were substantial. While the dpeak took about 4 minutes per plates, the  fastest algorithm took as little as 5 seconds per plate (more than a 60x speedup compared to the benchmark) and the slowest was well below one minute per plate. We observed no particular trade-off between speed and accuracy. The fastest algorithm, based on a gaussian mixture model, achieved a high accuracy as it ranked second overall. On the other hand, the algorithm with the best performance in terms of accuracy, which was based on a decision tree regression, also achieved a decent speed performance compared to the benchmark (a 17x speedup). 

## Ensembles

The observed dramatic reductions in computational time made worthy of consideration the implementation of  ensembles composed of different computational methods. For the present analysis, we assessed the performance of such ensembles by focusing on the subset of the data with shRNA experiments (ignoring the data with compound experiments). For this subset, the competitors achieved a significantly lower correlation and AUC than the data with compound experiments. We used a basic approach to aggregate the predictions of each solution by using the median prediction. Then, we tested the performance using the Spearman correlation and the AUC metrics computed on the holdout dataset for that plate. 

Results (**FIGURE REF**) showed that the performance of the ensemble tended to increase with the number of models involved. However, the maximum performance in both metrics tended to plateau (or even decrease) after the ensamble reaches a size equal to 3 models. This result suggests limited gains from having ensembles, although the analysis may be limited to using the median prediction for aggregation.
