# Results

The contest attracted 294 participants who made 820 submissions using a variety of different methods (\nameref{s1-table}). 
We report the top four methods based on the holdout data. 

The winning solution (by a competitor from the United States with a degree in Physics from the University of Kansas) used a random forest algorithm. The algorithm combines predictions from 10 different trees trained on 60 derived data features. These features include a combination of low-peak and high-peak estimates for each gene pair and aggregate measures that are sensitive to systematic bias at the perturbagen, analyte, and plate level.

The second solution (by a competitor from Poland with a Master's degree in Computer Science from Lodz University of Technology) used the Expectation-Maximization (EM) algorithm to fit a mixture of two log-normal models to each gene pair's data. This algorithm does not assume any a priori probability (the 2:1 ratio) of assignment to clusters, but learns it from the data by fitting a plate-wide distribution of cluster sizes. 
 
The third solution (by a competitor from India with a bachelor's degree in Computer Science) used a fast k-means algorithm with a random initialization procedure that tends to avoid local minima and is more robust to extreme outliers.
  
The fourth solution (by a competitor from Ukraine with a bachelor's degree in Computer Science from the Cherkasy National University) used a Convolutional Neural Network (CNN). This algorithm first filters and transforms the data into a 32-bin histogram for each pair of genes. Then, it uses the U-net architecture [@ronneberger2015u], comprising a contracting path to capture context and a symmetric expanding path, to provide adequate representation of the data. The output of this network is then used to label bins into one of the two genes for each pair, and to predict the exact value within the bin. This second step uses two subnetworks with the same architecture and a mean squared error loss function.

## Clustering by method and perturbagen type

We evaluated the top-nine performing methods on the accuracy of their predictions, as well as their speed. Using the holdout dataset, we generated the contestants' deconvolution data (DECONV) and the corresponding differential expression (DE) values [as in @subramanian2017next]. We then compared the results using a two-dimensional t-SNE projection, run once on each of the entire DECONV and DE datasets [@maaten2008visualizing]. 

The DECONV data clustered well by pertubagen type and less by algorithm type (\Fig{figure-tsne}, A and B), although with some notable commonalities in the predictions generated by similar approaches (\Fig{figure-tsne}, C). For example, the decision tree regressor (DTR) algorithms have similar 'footprints' in the projection, as do the k-means and Gaussian mixture model (GMM) algorithms. After the transformation to DE data, however, the t-SNE projection was more homogenous, with no particular clustering by perturbagen and algorithm type (\Fig{figure-tsne} D), which was reassuring given the downstream analysis normally concentrates on DE values.

## Correlation accuracy

To evaluate the deconvolution accuracy, we used the genewise Spearman rank correlation ($\rho$) between the UNI data and the values obtained by the competitors through the deconvolution of DUO data. We did so for the shRNA and compound perturbagens separately, comparing the results between the subsets of 488 genes in high and low bead proportion. 

The winner's cumulative distribution of $\rho$'s was significantly shifted towards higher values compared to the benchmark's ($p<0.001$; \Fig{figure-corr}, a and b) with an average improvement that was twice as large with genes in low bead proportion compared to those in high bead proportion (5 and 2 percentage points, respectively). 

The other competitors showed similar improvements on average (\Fig{figure-corr}, c and d), although the average improvement was generally smaller compared to the winning method and, for the genes in low bead proportion, insignificantly different from the benchmark (\Fig{figure-corr}, c and d).

To evaluate the extent to which the winning algorithm outperformed the others, we ranked the top-nine algorithms by the average correlation metric for each genes (1 = highest, 9 = lowest). We then computed the percentage of genes for which a given algorithm was ranked first. The winner was ranked first for 30% of the genes, followed at some distance by the second-placed gaussian-mixture method (20%), and by the CNN method (13%). Thus, the top two submissions combined outperformed the rest for about half of the genes in our sample. Even so, all but a few algorithms were the best performers for at least 5% of the genes, suggesting some complementarity between these algorithms.

## Detection of extreme modulations

To evaluate the accuracy at the DE level, we used the detection accuracy of extreme modulations (genes notably up- or down-regulated by perturbation). We used the UNI data with DE values above a threshold as the ground truth; and we evaluated the detection accuracy of each solution by computing the corresponding AUC for each perturbagen type. The detection accuracy of extreme modulations was generally high for both shRNA and compound samples (AUC > 0.87 and AUC > 0.91,  respectively), with the competitors achieving notable improvements over the benchmark (\Fig{figure-auc}, a). Compared to the benchmark, the winning solution detects about 4 thousand less extreme modulations (40.8 and 44.2 thousand, respectively), thus being more conservative. However, when we restricted the comparison to extreme modulations detected by UNI as well (thus controlling for detection precision), the winning solution detects about 1.5 thousand more EMs than the benchmark (27.1 and 25.6 thousand, respectively), representing a sensible 6% increase in “true” EM detection.^[These results are for the dataset with shRNA experiments. We expect similar results for the dataset of compound experiments.]

We complemented the above analysis by using targeted gene knockdown (KD) experiments as the ground truth for a subset of data. These are experiments in which a landmark gene was targeted by an shRNA, and hence we expect to observe a significant decrease in expression for the targeted gene. We evaluated the KD detection accuracy of each solution by computing the corresponding percentage of successful KD genes identified or recalled by the algorithm (defining a successful KD as one gene in which the DE value and the corresponding gene-wise rank in the experiment are less than a given threshold, -2 and 10 respectively). We computed the percentage recall for the UNI data as well, which yielded an estimate of the maximum achievable recall of 0.80. Relative to this level, nearly all algorithms achieved a good recall and precision, with values that were higher than the benchmark solution for all but two methods (\Fig{figure-auc}, b). 

## Reduced variation across replicate samples

To evaluate the reproducibility of the results, we leveraged the several replicate samples for each shRNA and compound experiment in our dataset (about 4 and 10 replicates, respectively). We computed the mean gene-wise coefficient of variation (CV) for each method, which is a measure of inter-replicate variability computed as the average ratio between the interquartile range and the median value  across all the replicates. Using this measure, we found all solutions achieved significant improvements over the benchmark (\Fig{figure-inter}); and the winning method, which was the most accurate on average, also achieved the lowest inter-replicate variation overall.

## Computational speed

The speed improvements over the benchmark were substantial. While dpeak took about 4-5 minutes per plate, the  fastest algorithm took as little as 5 seconds per plate (more than a 60x speedup compared to the benchmark) and the slowest was well below one minute. These speed improvements are not directly attributable to the use of multiple cores, since both the benchmark and contestant algorithms leverage multi-core techniques. We observed no particular trade-off between speed and accuracy.

## Ensembles

Lastly, we assessed the performance of ensembles combining the predictions of different computational methods by taking the median value. By focusing on the subset of the data with shRNA experiments (ignoring the data with compound experiments), the performance in both Spearman correlation and the AUC metrics of the ensemble tended to increase with the number of models involved (\Fig{figure-ensembles}). However, the maximum performance in both metrics tended to plateau (or even decrease) after combining the results of 3 or more models. This result suggested limited gains from having ensembles, although it may be worth exploring more sophisticated aggregation approaches.
