# Results

The contest attracted 294 participants who made 820 code submissions overall with a variety of different analysis methods (see \nameref{table-1} for an overview of the best methods). Here we report the top-four performing methods based on the score obtained on the holdout data. 

The winning solution (by a competitor from the United States with a degree in Physics from the University of Kansas) was a random forest algorithm (combining 10 different trees). Each tree was trained using 60 different features of the data; 50 features were from the distribution of measurements within the gene pair, the remaining 10 features were measures of the association between the distribution of values within the pair and aggregate values at the plate and experiment level.

The second solution (by a competitor from Poland with a Master's degree in Computer Science from The Lodz University of Technology) used the Expectation-Maximization (EM) method for an initial clustering of the observed values, assuming the distribution was a mixture of two log-normal distributions (Gaussian mixture model). It then fit a plate-wide distribution of the estimated expressions and cluster sizes for each gene, using the results to adjust the assignment of peaks to the low- and high-proportion genes. 
 
The third solution (by a competitor from India with a bachelor's degree in Computer Science) was a  basic k-means algorithm with random initial points to avoid local minima along with other fine adjustments to minimize the impact of extreme outliers and improve speed.
  
The fourth solution (by a competitor from Ukraine with a bachelor's degree in Computer Science from the Cherkasy National University) was a Convolutional Neural Network (CNN). The algorithm first filtered the data to remove extreme outlier measurements, transforming the filtered data into a 32-bin histogram for each pair of genes. The CNN consisted of two parts. The first part used the classic U-net architecture [@ronneberger2015u] (a contracting path to capture context and a symmetric expanding path) to provide adequate representation of the data. The output of this network was then used to label bins into one of the two genes for each pair, and to predict the exact value within the bin. This second step was achieved using two subnetworks with the same architecture but weights trained separately (using a mean squared error loss function).

## Clustering by method and perturbagen type 

Given the variety of methods represented amongst the prize-winning solutions, we sought to assess whether there were notable differences in the predictions generated. Using the holdout dataset, we generated the contestants' deconvolution data (DECONV) and the corresponding differential expression (DE) values. We obtained the DE values by using the robust z-score procedure by @subramanian2017next. We then compared the results using a two-dimensional t-SNE projection [@maaten2008visualizing]. 

For the most part, the DECONV data clustered by pertubagen type, except for the ground truth UNI data, which appears distinct from the deconvolution samples (\Fig{figure-3}, A and B). 
Separating the samples by method revealed commonalities in the predictions generated by similar approaches (\Fig{figure-3}, C). For example, the decision tree regressor (DTR) algorithms have similar 'footprints' in the projection, as do the k-means and Gaussian mixture model (GMM) algorithms. This suggests that in general similar algorithms generate predictions with similar properties. However, after the transformation to DE data, the t-SNE projection appears more homogenous (\Fig{figure-3} D), indicating that perturbagen-type and algorithm-specific effects have greatly reduced. The reduced clustering at the DE level is reassuring given the downstream analysis is often focused on DE values.

## Correlation accuracy

We tested the accuracy of the solutions by comparing the distribution of the genewise Spearman's rank correlation ($\rho$) between the UNI data and the values obtained by the competitors through the deconvolution of DUO (holdout) data. We did so for the shRNA and compound experiments separately; and, within each set of experiments, we compared the correlation accuracy for genes in high and low bead proportion. In all these cases, we found that the winner's distribution was shifted to the right of the benchmark's, indicating significant improvements in accuracy. We also observed that, for each set of experiments, the greatest shift was for the distribution of genes in low bead proportions (compared to genes in high bead proportions). For the genes in low bead proportions, the winner's average improvement was of 5 percentage points (with standard error of half a point) whereas the improvement was of 2 percentage points (with standard error of half a point) for the genes in high bead proportions. 

The analysis of the top-nine performing algorithms showed similar results: the average Spearman's rank correlation was generally greater for genes in high bead proportions (relative to low bead proportions) and for genes in shRNA experiments (relative to compounds); the benchmark performed consistently worse than all the others in the genes in low bead proportions, although it performed well in the high-proportion genes. Remarkably, the winning solution outperformed the other approaches in nearly all the comparisons.

Next, we ranked the top-nine algorithms by the average correlation metric for each of the 976 landmark genes and selected the best algorithm for each gene across all the experiments. The winning solution was selected as the best performer for about a third (30%) of the genes. The second-placed gaussian mixture model was selected as the best performer for about one fourth (20%). Thus, the top two submissions achieved the highest correlation in more than half of the genes in our sample. Even so, all but a few algorithms were the best performers for at least 5% of the genes, suggesting some complementarity between these algorithms.

## Detection of extreme modulations

We further tested the accuracy of the competitors' solutions on the detection of genes notably up- or down-regulated by perturbation, or _extreme modulations_. These genes exhibit exceedingly high (or low) DE values (above a given threshold). Using the UNI DE values as the ground truth, we evaluated the detection accuracy of each solution by computing the corresponding area under the curve (AUC).

All the solutions achieved a good average detection accuracy (AUC > .86) with significant improvements over the benchmark in both the shRNA and compound experiments (\Fig{auc}). 

We further tested differences in accuracy for targeted gene knockdowns (KD). These are experiments involving an shRNA targeting one of the 976 landmark genes. We expected the targeted gene to exhibit a very low DE value (highly negative). For each such experiment, we define a successful KD as one in which the DE value is less than -2 and the gene-wise rank is less than 10, meaning that the targeted gene achieves one of its lowest DE values in the experiment where it was targeted. We computed the KD success frequency, or recall, for each competitor algorithm as well as the benchmark and UNI data. We used the $UNI$ ground-truth data to estimate the maximum achievable recall, which in this case was $0.8$. We observed that all (but 2) algorithms achieved a higher KD success frequency than the benchmark solution. These results suggest that the algorithm improvements translate to improvements in biologically relevant metrics used in common applications of L1000 data.

## Reduced variation across replicate samples

Our data contain between 4 to 11 replicate samples for each perturbagen. Keeping low the inter-replicate variation is of crucial practical importance to biologists because it means higher reproducibility of results. With this goal in mind, the benchmark k-means solution is likely suboptimal because it does little to mitigate the discrepancy in variability between the genes measured with high and low bead proportions. 

We compared the inter-replicate variance of the predictions made by each method for genes in low and high bead proportions. As expected, we found significant improvements in the inter-replicate variance of the competitors over the benchmark with the difference being more visible for genes in low bead proportions. Among the competitors, the winning method, which was the most accurate on average, was also the most efficient achieving the lowest inter-replicate variance.

## Computational speed

<!-- \nameref{table-1} shows that  -->
The speed improvements over the benchmark were substantial. While the dpeak took about 4 minutes per plates, the  fastest algorithm took as little as 5 seconds per plate (a 60x speedup compared to the benchmark) and the slowest was well below one minute per plate. We observed no particular trade-off between speed and accuracy. The fastest algorithm (`Ardavel`), that was based a gaussian mixture model, achieved a good level of accuracy as well, and ranked second overall. On the other hand, the algorithm with the best performance in terms of accuracy (`gardn999`), which was based on a decision tree regression, also achieved a decent speed performance compared to the benchmark. 


### Ensembles

Next, to assess the complementarity of the algorithms, we assessed the performance of ensembles composed of different number of models. For the present analysis, we focused on the subset of the data with shRNA experiments (ignoring the data with compound experiments). For this subset, the competitors achieved a significantly lower correlation and AUC  than the data with compound experiments. We used a basic approach to aggregate the predictions of each solution by using the median prediction. Then, we tested the performance using the Spearman correlation and the AUC metrics computed on the holdout dataset for that plate. Results (\Fig{ensemble}) showed that the performance of the ensemble tends to increase with the number of models involved. However, the maximum performance in both metrics tends to plateau (or even decrease) after the ensamble reaches a size equal to 3 models. 



