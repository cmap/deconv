


Introduction
============

Deconvolution problems are commonplace in many areas of science and engineering. In the context of biomedical research, a recurring issue is how to isolate signals of distinct populations (cell types, tissues, and genes) from composite measures obtained by a single analyte or sensor. This problem often stems from the prohibitive cost of profiling each population separately [@subramanian2017next; @cleary2017efficient] and has important implications for the analysis of transcriptional data in mixed samples [@shen2010cell; @zhong2012gene; @newman2015robust; @zaitsev2019complete], single-cell data [@deng2019scalable], and the study of cell dynamics [@lu2003expression], but it also appears in the analysis of imaging data [@preibisch2014efficient]. 

Existing computational deconvolution approaches work well in many specific settings [@shen2013computational] but they might be suboptimal in more general applications. Machine learning techniques can potentially improve upon current methods. A typical advantage is the ability to capture complicated patterns that can be hard to model otherwise, especially in complex and massive datasets as those frequently used in biomedical research. However, introducing machine learning in the field presents several challenges; some of which are validation, adaptation to complex datasets, and identification of the best machine-learning approaches to specific problems.

Here we describe how we addressed these challenges in the context of the Connectivity Map (CMap). The CMap project is a catalog of over 1.3 million human gene-expression profiles of genetic or pharmacologic perturbation. This resource enables rapid hypothesis development in multiple areas of biomedical research, including drug discovery and development [@subramanian2017next]. CMap has achieved its massive scale by using a new assay, called L1000. This assay uses a 500-plex fluorescence-based flow cytometry system to measure a reduced representation of the transcriptome comprised of 1,000 "landmark" genes that largely capture the cell's transcriptional state. L1000 achieves a significant cost reduction compared to more traditional methods, such as RNA-sequencing. However, a key technical challenge is to deconvolute the 500 measurements into separate 1,000 gene expression values. The current approach, a k-means algorithm called "dpeak", is slow and can be susceptible to errors. Below we report new methods, obtained via an open innovation challenge, that represent improvements to the current algorithm.

For the evaluation of these new methods, we generated a novel experimental dataset of L1000 profiles for 122 different perturbagens (shRNA and compounds) and several replicates for a total of over 2,200 gene expression experiments. We varied the detection mode for acquiring L1000 data between the current dual-detection procedure (DUO) that obtains a raw composite measure of two genes per analyte color, and a more expensive uni-detection procedure (UNI) that measures one gene per analyte color. The deconvolution algorithm processes the DUO data and assigns the correct expression level to each of the two genes whose transcripts bind to beads of the same analyte color. This procedure is not needed with the UNI data. Hence, the UNI data served as "ground truth" that enabled us to evaluate different deconvolution methods applied to the DUO data.

Leveraging this data set, we then explored different deconvolution approaches through an open innovation competition [@lakhani2013prize; @blasco2019advancing]. We ran the contest on Topcoder (Wipro, India), a popular crowdsourcing platform. The contest challenged participants to use the novel dataset to improve the deconvolution algorithm utilized by the L1000 platform. The contest drew about 300 competitors from 20 different countries and resulted in a diversity of approaches. The top approaches included machine-learning methods, such as Random Forests and Convolutional Neural Networks (CNNs), as well as more traditional Gaussian-mixture models and k-means. These approaches significantly performed better than the L1000 benchmark in various measures of accuracy and computational speed, and likely have application beyond gene expression.


<!-- 
TO READ: 

"DeconvSeq: deconvolution of cell mixture distribution in sequencing data" https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btz444/5506629?redirectedFrom=fulltext

"Evaluation of methods to assign cell type labels to cell clusters from single-cell RNA-sequencing data [version 3; peer review: 2 approved, 1 approved with reservations]"
https://f1000research.com/articles/8-296

Systematic comparative analysis of single cell RNA-sequencing methods
https://www.biorxiv.org/content/10.1101/632216v1

The Technology and Biology of Single-Cell RNA Sequencing
https://www.sciencedirect.com/science/article/pii/S1097276515002610

Fractional proliferation: a method to deconvolve cell population dynamics from single-cell data
https://www.nature.com/articles/nmeth.2138

Bulk tissue cell type deconvolution with multi-subject single-cell expression reference
https://www.nature.com/articles/s41467-018-08023-x

A Single-Cell Transcriptomic Map of the Human and Mouse Pancreas Reveals Inter- and Intra-cell Population Structure
https://www.sciencedirect.com/science/article/pii/S2405471216302666

-->