
Introduction
============

Deconvolution problems are commonplace in many areas of science and engineering. In the context of biomedical research, a typical example is how to isolate specific gene expressions of distinct populations (cell types, tissues, and genes) from composite measures obtained by a single analyte or sensor. This problem often stems from the prohibitive cost of profiling each population separately [@subramanian2017next; @cleary2017efficient] and has important implications for the analysis of transcriptional data in mixed samples [@shen2010cell; @zhong2012gene; @newman2015robust; @zaitsev2019complete], single-cell data [@deng2019scalable], the study of cell dynamics [@lu2003expression], but it also appears in the analysis of imaging data [@preibisch2014efficient]. 

Existing deconvolution approaches work well in many specific settings, but might be suboptimal to the increasing availability of biological data. Machine learning techniques can potentially improve upon current methods to identify and isolate measurements from distinct populations; a typical advantage is the ability to capture automatically complicated patterns that can be hard to model otherwise (especially in complex and massive datasets as those frequently used in biomedical research). However, introducing machine learning in the field presents several challenges; some of which are validation, adaptation to complex datasets, and identification of the best machine-learning approaches to specific problems.

Here we describe how we addressed these challenges in the context of the Connectivity Map (CMap), which is a massive catalogue of human gene-expression signatures of genetic or pharmacologic perturbation that includes more than 1.3 million publicly accessible profiles, with lots of applications in drug discovery and development [@subramanian2017next]. 

The CMap's team uses a new gene-expression assay, called L1000, to efficiently generate a high volume of profiles. L1000 is a high-throughput gene expression profiling assay that measures the mRNA transcript abundance of a subset of approximately 1000 genes highly representative of the state of human cells [@subramanian2017next]. By doing so, L1000 achieves significant cost reductions compared to more traditional methods, such as RNA-sequencing. Part of L1000's cost reductions stem from a deconvolution procedure, called d-peak, that isolates gene-specific expressions of two genes from a composite measure obtained by a single analyte type. This procedure works well in practice, but might still be suboptimal for this problem.

Using L1000, we generated a novel experimental dataset with the transcriptional response of approximately 1,000 genes to 122 different perturbagens (shRNA and compounds) and several replicates for a total of over 2,200 gene expression experiments. For each experiment, we used L1000 with two different detection methods. The first detection method, called UNI, acquired the gene expression of each single gene with one single analyte type (approximately 1,000 $\times$ 2,200 = 2,200,000 distinct measurements). 
The second method, called DUO, acquired a composite measure of the gene expression of two genes by coupling each analyte type to two different genes (approximately 500 $\times$ 2,200 = 1,100,000 distinct measurements). Because the experimental conditions were nearly identical between the two detection methods, the UNI data provided ground truth that enabled us to evaluate empirically different deconvolution methods on the DUO data.

Leveraging this data set, we then explored different machine learning approaches through an open innovation competition, or contest, which we designed following insights from our previous work [@lakhani2013prize; @blasco2019advancing]. We run the contest on Topcoder, a popular crowdsourcing platform. The contest challenged participants to use the novel dataset to improve the deconvolution algorithm utilized by the L1000 platform. The contest drew about 300 competitors from across the globe and resulted in a diverse number of machine-learning approaches. The top approaches included machine-learning methods, such as Random Forests and Convolutional Neural Networks (CNNs), as well as more traditional Gaussian-mixture models. These approaches significantly performed better than the L1000 benchmark in various measures of accuracy and computational speed, and likely have application beyond gene expression.


<!-- 
TO READ: 

"DeconvSeq: deconvolution of cell mixture distribution in sequencing data" https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btz444/5506629?redirectedFrom=fulltext

"Evaluation of methods to assign cell type labels to cell clusters from single-cell RNA-sequencing data [version 3; peer review: 2 approved, 1 approved with reservations]"
https://f1000research.com/articles/8-296

Systematic comparative analysis of single cell RNA-sequencing methods
https://www.biorxiv.org/content/10.1101/632216v1

The Technology and Biology of Single-Cell RNA Sequencing
https://www.sciencedirect.com/science/article/pii/S1097276515002610

Fractional proliferation: a method to deconvolve cell population dynamics from single-cell data
https://www.nature.com/articles/nmeth.2138

Bulk tissue cell type deconvolution with multi-subject single-cell expression reference
https://www.nature.com/articles/s41467-018-08023-x

A Single-Cell Transcriptomic Map of the Human and Mouse Pancreas Reveals Inter- and Intra-cell Population Structure
https://www.sciencedirect.com/science/article/pii/S2405471216302666

-->