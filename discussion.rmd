Discussion
==========

Motivated by recent successes in the use of machine learning techniques to do x, y and ,z, we hypothesized that better deconvolution approaches could be developed. To test this hypthesis, we first generated a novel experimental dataset with differential-expression measurements obtained by two different detection methods. The first that involves deconvoltion, which we used as the ground-truth, and another that measures 2 genes per analyte, which was used a training dataset. Then, we run an open innovation competition to enable a cost-effective xxx of the space of possible solutions using these experimental data. We report the results of the this challenge.  


The top three approaches as they emerged from the challenge included different machine-learning methods such as Random Forests, ConvNet, and Gaussian Mixtures. We evaluted these methods on the holdout data. 
The developed methods achieved significant improvements in accuracy (correlation) of baseline gene expression values, as well as a better performacne in the detection of extremely modulated genes (e.g., xxxx). Compared to the benchmark, these developed methods were more consistent across replicates (a smaller inter-replicate variability), thus leading to more reliable predictions overall. 

We evaluated the computational speed of all the approaches. Overall, we find small speed vs accuracy tradeoff. 
Parametric methods, such as Gaussian Mixture, achieved 60x speedup compared to the benchmark, without losing on the accuracy. Machine-learning methods, such Random Forest, achieved greater accuracy but were slower. Yet, the ratio of accuracy over speed improvement was relatively small. 

Motivated by the success of ensamble methods, we evaluated possible complementarity between the different approaches. 
We built an algorithm that combines the top methods selected by gene based on the results on the training data. 
On the holdout, we found that the ensamble further improved accuracy relative to xxxx.


## Inspection of methods.

- gardn999. This was the winner, who holds a PhD In Physics from the University of Kansas. His solution was a Random Forest based on a total of 60 "features." These features included the actual values (50 variables?) and "various relationships of the averaged quantities in the Barcode, Experiment and Set for which the Pair belongs." The model was training using a total of 10 trees, to improve computational speed. Accuracy could be improved by increasing the number of trees. 

- Ardavel used the EM algorithm for each pair and for a plate-wide distribution of expression and cluster size that is used for adjusted (especially of low-proportion genes); as well as further per-plate per-well adjustements. Find that it is better to do not assume a priory a probability of cluster size (i.e., 2:1 ratio), but it is better to estimate it from the data. Found many 3 peaked. Realized that plate-wide information was crucial to improve the solution, indicating where to swap the peaks and indicate false peaks. 

- mkgenious used basic k-means tools tailered to the data to increase speed and accuracy. In other words, this was a fine-tuned implementation of the benchmark. 

- Ramsez2  focused on the prediction of extreme modulations using as input 32bins histogram of the measurements for each pair. Neural Net predicts low and high values separately (2 subnetworks same architecture but trained separately). See figure. Used MSE loss and cross-entropy with Adam optmizer. 

## Future work. 

- We have created a dataset of over 120 shRNA and compound experiments with measurements for about 1000 genes. This dataset constitutes a public resource to all the researchers in this area who are interested in testing their deconvoltuion appproaches. 
- However, it remains to be seen performance on combining thre or more genes with single analytes. This is future work. 
- Next, we will apply these resutls to over one million experiments  and explore cost svings achieved by having a lower number of replicates



## Older notes 

Summary of the results presented in the methods section. 

Discussion generality of the solutions

- Novel? Have any of these solutions previously been applied to deconvolution problems?
- Specific to this problem or general to others?

Discuss implications of these methods for CMap production

- Preliminary results on past data conversion
- Directions for pipeline integration and generation of future data
- Cost savings
- Implementation strategy and outcomes
- Increase in data processing throughput

