---
title: "Improving Deconvolution Methods in Biology \
        through Open Innovation Competitions: \
        an Application to the Connectivity Map"

author: 
    - "[INCOMPLETE, ORDER TBD]"
    - Andrea Blasco (Harvard)
    - Ted Natoli (Broad)
    - Rinat A. Sergeev (Harvard)
    - Jin H. Paik (Harvard)
    - Max Macaluso (Broad)
    - Rajiv Narayan (Broad)
    - [...]
    - Karim R. Lakhani (Harvard)
    - Aravind Subramanian (Broad)

abstract: "Deconvolution methods are ubiquitous and have a long history in many areas of science and engineering.   A common problem of deconvolution algorithms is the validation of results using realistic datasets for testing, and the costs involved in tailoring different approaches to such datasets. We describe the deconvolution problem of an assay, L1000, that measures mRNA transcript abundance of approximately 1000 genes from human cells using 500 Luminex beads, such that two transcripts are identified by a single bead color. Using L1000, we generated a novel dataset of transcripts for various perturbagens (shRNA and compounds) where genes were measured both in tandem and individually. We then ran an open competition to elicit different deconvolution algorithms. The competition produced a wide variety of techniques including machine-learning algorithms, such as Convolutional Neural Networks and Random Forests, and more traditional approaches, such as Gaussian mixtures and k-means. All top solutions achieved notable improvements over the benchmark." 

date: 'Last updated: `r format(Sys.Date(), "%b %d, %Y")`'

keywords:
  - biology
  - open innoation competitions
  - crowdsourcing
  - deconvolution
  - gene expressions
  - cell lines

titlepage: false 
linestretch: 1.25
fontfamily: mathpazo
# fontsize: 11pt
# colorlinks: true
# urlcolor: cyan
# citecolor: blue 
geometry: margin=1.25in
# endfloat: true
bibliography: dpeak.bib
biblatexoptions: "sorting=none"
header-includes:
  - \usepackage{bbm}
  - \newcommand\hl{}
  - \newcommand\Fig[1]{{\it Fig. \ref{fig:#1}}}
  - \usepackage{setspace}
  - \onehalfspacing 
---


TODOS:

- Intro: Write paragraph 1 for the actual journal submission.
- Results: Describe motivation for each metric  
    - assessing "global similarity" b/w predictions and ground truth.
    - assessubg EM detection
- Figures: drop panel C
- Address comments at the end of manuscript


```{r sections, include = FALSE}
# "instructions.rmd" 
childs <- c("intro.rmd", "methods.rmd"
          , "results.rmd", "discussion.rmd", "figures.rmd", "supporting.rmd")
```

```{r child=childs}
``` 


## Comments

### Rinat's 

1. There is some odd figure ordering in the draft. Figure 1 is referenced only at the very last in Appendix (thus, should be an appendix figure); Then there are Figs 3, 6, 4, ??=5, and 7. Easy fix - just not to forget 
2. Similarly, the table of contents of the section 4. Figures does not match the list of figures. It is also better to use Figure 1, Figure 2 in that list - instead of 4.1, 4.2, 4.3â€¦
3. As for the figures, I wonder if there is a better way to distinguish the algorithm performances than comparing their CDFs which are currently barely distinguishable on the plots. May be, showing the difference:
F_benchmark[p] - F_algorithm[p]? Or even:
P_algorithm[F[p]]-P_benchmark[F[p]]?
Just an idea - but it may over-complicate things, so we may just want to keep it as it is.
4. As for the ensemble, more accurate way of estimating the complementarity of the algos would be to use a vector of weights (w_i), compute the ensemble predictions as a sum: P_ensemble = Sum[ w_i * P_i], then derive w_i that maximize the Score[P_ensemble] on testing dataset, and then compute the score on the holdout one.
The advantage of that is that:
a) you get realistic ensemble performance, that is expected to always increase (saturating -> diminishing returns) with more algorithms included.
b) you obtain the explicit values of w_i - weights of the algos, that would represent their add-on value. Then you can state something smart about w_i decreasing with i, and possibly higher paired w_ij for those algorithms i and j that are produced by different approaches (to strengthen the narrative around Table S3 - category).
Again - just an idea. We can drop it and keep as it is.
5. I wonder if on Figure 5 it is better to sort the contestants by their placement rather then by their runtime - that would better illustrate the statement of low correlation between performance and runtime. It would also make sense to add a category (Table S3) mark to this plot - as the runtimes may be heavily clustered by the method.
Also, I would probably use the log-scale for Y-axes on those plots.
Again - the draft looks really great! Great job!!!


References
==========

