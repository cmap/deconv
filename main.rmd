---
title: "Improving Deconvolution Methods in Biology \
        through Open Innovation Competitions: \
        an Application to the Connectivity Map"

author: 
    - Andrea Blasco (Harvard)
    - Ted Natoli (Broad)
    - Rinat A. Sergeev (Harvard)
    - Steven Randazzo (Harvard)
    - Jin H. Paik (Harvard)
    - Max Macaluso (Broad)
    - Rajiv Narayan (Broad)
    - Karim R. Lakhani (Harvard)
    - Aravind Subramanian (Broad)

abstract: "A central goal in biology is to understand the response of human cells to perturbations. Researchers pursuing this goal often seek to characterize such cellular responses using large repositories of gene-expression profiles, such as the Connectivity Map. However, because of the prohibitive cost of profiling many genes or cells separately, they must first deconvolute gene-expression profiles of distinct populations (cell types, tissues, and genes) from composite measures obtained by a single analyte, or sensor. To improve upon existing solutions, we generated a novel dataset of gene expression profiles of 2,2000 perturbations (shRNA and compounds) where genes were measured both separately and in tandem. We then ran an open competition challenging participants to develop deconvolution solutions using these data. The contest attracted 300 competitors worldwide who employed both machine learning (Convolutional Neural Networks and Random Forests) and more traditional approaches (Gaussian mixtures and k-means). Through the analysis of the top-performing methods, we found that the winning approach, a random forest regressor, achieved the highest global correlation with the ground truth, the lowest inter-replicate variability, and, compared to the k-means benchmark solution, was able to detect more than a thousand additional differentially-expressed genes while also improving precision. This provides evidence of the tremendous potential for using random-forest approaches for deconvolution problems in biology."

date: 'Last updated: `r format(Sys.Date(), "%b %d, %Y")`'

keywords:
  - biology
  - open innoation competitions
  - crowdsourcing
  - deconvolution
  - gene expressions
  - machine learning

fontsize: 11pt
linestretch: 1.5
fontfamily: mathpazo
geometry: margin=1in
# titlepage: false 
# colorlinks: true
# urlcolor: cyan
# citecolor: blue 
# endfloat: true
bibliography: dpeak.bib
biblatexoptions: "sorting=none"
header-includes:
  - \usepackage{bbm}
  - \newcommand\hl{}
  - \newcommand\Fig[1]{{\it Fig. \ref{fig:#1}}}
  - \usepackage{setspace}
  - \onehalfspacing 
---


<!-- 
TODOS:
- Intro: Write paragraph 1 for the actual journal submission.
- Results: Describe motivation for each metric  
    - assessing "global similarity" b/w predictions and ground truth.
    - assessubg EM detection
- Figures: drop panel C
- Address comments below

1. There is some odd figure ordering in the draft. Figure 1 is referenced only at the very last in Appendix (thus, should be an appendix figure); Then there are Figs 3, 6, 4, ??=5, and 7. Easy fix - just not to forget 
2. Similarly, the table of contents of the section 4. Figures does not match the list of figures. It is also better to use Figure 1, Figure 2 in that list - instead of 4.1, 4.2, 4.3â€¦
3. As for the figures, I wonder if there is a better way to distinguish the algorithm performances than comparing their CDFs which are currently barely distinguishable on the plots. May be, showing the difference:
F_benchmark[p] - F_algorithm[p]? Or even:
P_algorithm[F[p]]-P_benchmark[F[p]]?
Just an idea - but it may over-complicate things, so we may just want to keep it as it is.
4. As for the ensemble, more accurate way of estimating the complementarity of the algos would be to use a vector of weights (w_i), compute the ensemble predictions as a sum: P_ensemble = Sum[ w_i * P_i], then derive w_i that maximize the Score[P_ensemble] on testing dataset, and then compute the score on the holdout one.
The advantage of that is that:
a) you get realistic ensemble performance, that is expected to always increase (saturating -> diminishing returns) with more algorithms included.
b) you obtain the explicit values of w_i - weights of the algos, that would represent their add-on value. Then you can state something smart about w_i decreasing with i, and possibly higher paired w_ij for those algorithms i and j that are produced by different approaches (to strengthen the narrative around Table S3 - category).
Again - just an idea. We can drop it and keep as it is.
5. I wonder if on Figure 5 it is better to sort the contestants by their placement rather then by their runtime - that would better illustrate the statement of low correlation between performance and runtime. It would also make sense to add a category (Table S3) mark to this plot - as the runtimes may be heavily clustered by the method.
Also, I would probably use the log-scale for Y-axes on those plots.
Again - the draft looks really great! Great job!!!
 -->

```{r global-options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
childs <- list.files(pattern = "^[0-9].*rmd")
```

```{r child=childs}
``` 

References
==========

