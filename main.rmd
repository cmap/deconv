---
title: "Improving Deconvolution Methods in Biology \
        through Open Innovation Competitions: \
        an Application to the Connectivity Map"

author: 
    - Andrea Blasco (Harvard)
    - Ted Natoli (Broad)
    - Michael G. Endres (Harvard)
    - Rinat A. Sergeev (Harvard)
    - Steven Randazzo (Harvard)
    - Jin H. Paik (Harvard)
    - Max Macaluso (Broad)
    - Rajiv Narayan (Broad)
    - Karim R. Lakhani (Harvard)
    - Aravind Subramanian (Broad)

abstract: "The Connectivity Map (CMap) project has generated millions of perturbational gene expression profiles using a new assay, called L1000.  Using computational deconvolution methods, the L1000 platform characterizes the expression of 1,000 genes from composite measurements obtained by 500 analytes. This procedure greatly lowers cost allowing CMap's massive scale-up. However, the current deconvolution algorithm is slow (~5 minutes per experimental plate) and susceptible to errors. Here we describe new methods obtained via an open innovation challenge. The challenge used a novel dataset of 2,200 L1000 profiles of perturbation experiments for evaluation and attracted 294 competitors from 20 countries. The top-nine performing methods ranged from machine learning (Convolutional Neural Networks and Random Forests) to more traditional solutions (Gaussian Mixtures and k-means). These approaches were faster and more accurate than the benchmark and likely have applications beyond gene expression."
    
date: 'Last updated: `r format(Sys.Date(), "%b %d, %Y")`'

keywords:
  - connectivity map
  - open innovation competitions
  - crowdsourcing
  - deconvolution
  - gene expressions
  - machine learning
  - transcriptome

geometry: margin=1in
bibliography: dpeak.bib
biblatexoptions: "sorting=none"

header-includes:
  - \usepackage{bbm}
  - \newcommand\Fig[1]{\nameref{#1}}
  - \usepackage{setspace}
  - \onehalfspacing 
---


<!-- abstract: "A central goal in biology is to understand the response of human cells to perturbations.  Researchers pursuing this goal often seek to characterize cellular responses using large repositories of gene-expression profiles, such as the Connectivity Map. However, a recurrent issue is how to isolate specific gene expressions of distinct populations (cell types, tissues, and genes) from composite measures obtained by a single analyte or sensor, which stems from the prohibitive cost of profiling each population separately. To improve upon existing solutions, we generated a novel dataset of gene expressions from 2,2000 perturbation experiments where genes were measured both separately and in tandem. We then ran an open competition challenging participants to develop deconvolution solutions using these data. The contest attracted 300 competitors worldwide who employed both machine learning (Convolutional Neural Networks and Random Forests) and more traditional approaches (Gaussian mixtures and k-means). The winning approach, a random forest regressor, achieved the highest global correlation with the ground truth, the lowest inter-replicate variability, and, compared to the k-means benchmark solution, was able to detect more than a thousand additional differentially-expressed genes while also improving precision. This provides evidence of the tremendous potential of random-forest approaches for deconvolution problems in biology."
 -->

<!-- 
TODOS:
- Intro: Write paragraph 1 for the actual journal submission.
- Results: Describe motivation for each metric  
    - assessing "global similarity" b/w predictions and ground truth.
    - assessubg EM detection
- Figures: drop panel C
- Address comments below

1. There is some odd figure ordering in the draft. Figure 1 is referenced only at the very last in Appendix (thus, should be an appendix figure); Then there are Figs 3, 6, 4, ??=5, and 7. Easy fix - just not to forget 
2. Similarly, the table of contents of the section 4. Figures does not match the list of figures. It is also better to use Figure 1, Figure 2 in that list - instead of 4.1, 4.2, 4.3â€¦
3. As for the figures, I wonder if there is a better way to distinguish the algorithm performances than comparing their CDFs which are currently barely distinguishable on the plots. May be, showing the difference:
F_benchmark[p] - F_algorithm[p]? Or even:
P_algorithm[F[p]]-P_benchmark[F[p]]?
Just an idea - but it may over-complicate things, so we may just want to keep it as it is.
4. As for the ensemble, more accurate way of estimating the complementarity of the algos would be to use a vector of weights (w_i), compute the ensemble predictions as a sum: P_ensemble = Sum[ w_i * P_i], then derive w_i that maximize the Score[P_ensemble] on testing dataset, and then compute the score on the holdout one.
The advantage of that is that:
a) you get realistic ensemble performance, that is expected to always increase (saturating -> diminishing returns) with more algorithms included.
b) you obtain the explicit values of w_i - weights of the algos, that would represent their add-on value. Then you can state something smart about w_i decreasing with i, and possibly higher paired w_ij for those algorithms i and j that are produced by different approaches (to strengthen the narrative around Table S3 - category).
Again - just an idea. We can drop it and keep as it is.
5. I wonder if on Figure 5 it is better to sort the contestants by their placement rather then by their runtime - that would better illustrate the statement of low correlation between performance and runtime. It would also make sense to add a category (Table S3) mark to this plot - as the runtimes may be heavily clustered by the method.
Also, I would probably use the log-scale for Y-axes on those plots.
Again - the draft looks really great! Great job!!!
 -->

```{r global-options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
childs <- list.files(pattern = "^[0-9].*rmd")
```

```{r child=childs}
``` 

References
==========

