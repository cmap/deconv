---
title: "Improving Deconvolution Methods in Biology \
        through Open Innovation Competitions: \
        an Application to the Connectivity Map"

author: 
    - Andrea Blasco (Harvard)
    - Ted Natoli (Broad)
    - Rinat A. Sergeev (Harvard)
    - Steven Randazzo (Harvard)
    - Jin H. Paik (Harvard)
    - Max Macaluso (Broad)
    - Rajiv Narayan (Broad)
    - Karim R. Lakhani (Harvard)
    - Aravind Subramanian (Broad)


abstract: "How to isolate specific gene expressions of distinct populations (cell types, tissues, and genes) from composite measures obtained by a single analyte or sensor? This is a typical problem in biology, a consequence of the prohibitive cost of profiling each population separately, that has important implications for biomedical research. To improve upon existing solutions, we generated a novel dataset with the transcriptional response of approximately 1,000 genes to 122 different perturbagens (shRNA and compounds) and several replicates for a total of over 2,200 gene expression experiments; we then used these data to explore different machine learning approaches through an open innovation competition; and we found that: (1) Random Forests, Convolutional Networks, as well as more traditional Gaussian Mixture models consistently outperform other common approaches, such as k-means clustering, in prediction accuracy; and (2) the winning approach, which was a Random Forest, achieved the highest global correlation with the ground-truth, the lowest inter-replicate variability, and, compared to the benchmark, was able to detect more than a thousand additional differentially-expressed genes, while improving the detection precision at the same time. This provides evidence of the tremendous potential of using random-forest approaches for deconvolution problems in biology."

date: 'Last updated: `r format(Sys.Date(), "%b %d, %Y")`'

keywords:
  - biology
  - open innoation competitions
  - crowdsourcing
  - deconvolution
  - gene expressions
  - cell lines
  - machine learning 

fontsize: 12pt
linestretch: 1.25
fontfamily: mathpazo
geometry: margin=1.5in
# titlepage: false 
# colorlinks: true
# urlcolor: cyan
# citecolor: blue 
# endfloat: true
bibliography: dpeak.bib
biblatexoptions: "sorting=none"
header-includes:
  - \usepackage{bbm}
  - \newcommand\hl{}
  - \newcommand\Fig[1]{{\it Fig. \ref{fig:#1}}}
  - \usepackage{setspace}
  - \onehalfspacing 
---


<!-- 
TODOS:
- Intro: Write paragraph 1 for the actual journal submission.
- Results: Describe motivation for each metric  
    - assessing "global similarity" b/w predictions and ground truth.
    - assessubg EM detection
- Figures: drop panel C
- Address comments below

1. There is some odd figure ordering in the draft. Figure 1 is referenced only at the very last in Appendix (thus, should be an appendix figure); Then there are Figs 3, 6, 4, ??=5, and 7. Easy fix - just not to forget 
2. Similarly, the table of contents of the section 4. Figures does not match the list of figures. It is also better to use Figure 1, Figure 2 in that list - instead of 4.1, 4.2, 4.3â€¦
3. As for the figures, I wonder if there is a better way to distinguish the algorithm performances than comparing their CDFs which are currently barely distinguishable on the plots. May be, showing the difference:
F_benchmark[p] - F_algorithm[p]? Or even:
P_algorithm[F[p]]-P_benchmark[F[p]]?
Just an idea - but it may over-complicate things, so we may just want to keep it as it is.
4. As for the ensemble, more accurate way of estimating the complementarity of the algos would be to use a vector of weights (w_i), compute the ensemble predictions as a sum: P_ensemble = Sum[ w_i * P_i], then derive w_i that maximize the Score[P_ensemble] on testing dataset, and then compute the score on the holdout one.
The advantage of that is that:
a) you get realistic ensemble performance, that is expected to always increase (saturating -> diminishing returns) with more algorithms included.
b) you obtain the explicit values of w_i - weights of the algos, that would represent their add-on value. Then you can state something smart about w_i decreasing with i, and possibly higher paired w_ij for those algorithms i and j that are produced by different approaches (to strengthen the narrative around Table S3 - category).
Again - just an idea. We can drop it and keep as it is.
5. I wonder if on Figure 5 it is better to sort the contestants by their placement rather then by their runtime - that would better illustrate the statement of low correlation between performance and runtime. It would also make sense to add a category (Table S3) mark to this plot - as the runtimes may be heavily clustered by the method.
Also, I would probably use the log-scale for Y-axes on those plots.
Again - the draft looks really great! Great job!!!
 -->

```{r global-options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
childs <- c("intro.rmd", "methods.rmd", "results.rmd", "discussion.rmd", "figures.rmd", "supporting.rmd")
```

```{r child=childs}
``` 

References
==========

