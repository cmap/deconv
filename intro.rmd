
Introduction
============

<!-- 
A major limitation of multianalytes assays is their limited number of available analyte types. 
In the context of RNA profiling, the CMap group has developed a framework that integrates statistical analysis with traditional multianalyte methods to infer differential expressions for multiple genes from the same analyte type. 
We report the results of an open innovation competition that impvored upon the state-of-the-art by integrating machine-learning techniques to address the deconvultion problem. 
 -->

<!--
Over the last decade, the ability to analyze multiple analytes simultaneously from the same biological sample has contributed to creating large perturbational datasets that are essential to accelerate the discovery of novel therapeutics. However, a fundamental limitation of traditional multianalyte assays lies in the restricted extent and type of available analytes
-->

Multiplex[ed] assays frequently use a single analyte to measure multiple populations ... 

RNA profiling is an essential means to learn about the state of cells and tissues in response to perturbations; it helps detecting similarities between different cellular states that, in turn, provide clues about therapeutic intervention, help the elucidation of moas, etc. However, gaining an understanding of genetic transcriptome perturbations will require extensive experimentation, which can be costly to generate at scale.

<!-- 
RNA profiling is an excellent phenotype of cellular responses and tissue states, but can be costly to generate at the massive scale required for studies of regulatory circuits, genetic states or perturbation screens. Here, we draw on a series of advances over the last decade in the field of mathematics to establish a rigorous link between biological structure, data compressibility, and efficient data acquisition. We propose that very few random composite measurements – in which gene abundances are combined in a random linear combination – are needed to approximate the high-dimensional similarity between any pair of gene abundance profiles. We then show how finding latent, sparse representations of gene expression data would enable us to “decompress” a small number of random composite measurements and recover high-dimensional gene expression levels that were not measured (unobserved). We present a new algorithm for finding sparse, modular structure, which improves the ability to interpret samples in terms of small numbers of active modules, and show that the modular structure we find is sufficient to recover gene expression profiles from composite measurements (with ~100-fold fewer composite measurements than genes). Moreover, the knowledge that sparse, modular structures exist allows us to recover expression profiles from composite measurements, even without access to any training data. Finally, we present a proof-of-concept experiment for making composite measurements in the laboratory, involving the measurement of linear combinations of RNA abundances. Altogether, our results suggest new compressive modalities in experimental biology that can form a foundation for massive scaling in high-throughput measurements, while also offering new insights into the interpretation of high-dimensional data.
 -->



A central problem 
Over the last decade, technological advances have made it possible to generate large perturbational datasets which in turn have helped accelerate therapeutic discovery. A large proportion of these technologies rely on using multiple analytes to measure biological signals. These assays are extremely powerful but in some applications are limited by the number of unique analytes they can detect.^[For example, Luminex's most advanced platform, called FLEXMAP 3D, can measure simultaneously a maximum of 500 bead types for an equal number of genes or proteins from a small sample.]

This limitation is more prominent for gene-expression profiling, given the high number of genes involved. Within this context, the Connectivity Map (CMap) has developed an assay, called L1000, that integrates traditional multianalyte detection methods and statistical analyses to simultaneously measure the expression of multiple genes using the same analyte type, thus multiplying the capacity of existing technologies [@subramanian2017next].

@cleary2017efficient proposes to use "compressed sensing to use low-level measurements or composite signals (from multiple genes) to recover high-level... 
@deng2019scalable
@hie2019efficient
such as to measure differential gene expression for each gene in mixed cell-type samples [@shen2010cell], 

<!-- The algorithm D-peak is based on k-means algorithm, a general and flexible tool to conduct non-parametric regression that is very popular in biology, as well as in many other fields.  -->
<!-- The current algorithm works well, but has some limitations.  -->
<!-- > OLD Multianalyte methods for high-throughput gene-expression profiling have found widespread application in biology. The extent of these applications, however, tend to be limited by the type and number of available analytes, which typically results in prohibitive costs for big data generation. Using an assay called L1000 that measures the mRNA transcript abundance of 978 "landmark" genes from human cells, the Connectivity Map (CMap) group at the Broad Institute has developed a novel approach that matches pairs of genes to the same analyte type, or bead, to double the count of profiled genes, thus lowering greatly costs [@subramanian2017next].  -->

A central component of this approach is the computational deconvolution of gene expression signals from multiple genes measured using the same analyte. Similar deconvolution problems are ubiquitous in biology and several solutions have been proposed in a variety of different contexts. Examples include algorithms to identify cell type–specific gene expression differences in complex tissues [@shen2010cell] or dynamic changes in cell populations [@lu2003expression], or ways to discover the target proteins of small molecules [@terstappen2007target].

CMap’s current deconvolution approach, called dpeak, is based on the k-means clustering algorithm, which is a flexible  unsupervised learning procedure widely used in biology, as well as in many other fields. 
This approach automatically partitions a set of gene-expression measurements into $k$ clusters by minimizing the within-cluster sum of squares. It then assigns the clusters to each gene within the sample (see @subramanian2017next for the details). This approach works well in practice but has several limitations; it tends to split clusters incorrectly when the distributions are not reasonably well separated, it is sensitive to outliers, and it is computationally demanding.^[An efficient k-means algorithm, such as Lloyd's, has a running time that scales as $O(kn)$ where $n$ is the number of observations and $k$ the number of clusters.]

Motivated by recent successes with machine learning techniques in biology, we hypothesized better deconvolution approaches that could likely be developed. To test this hypothesis, we generated a novel experimental dataset of the response of approximately 1,000 genes to 122 different perturbagens (shRNA and compounds) within 4 to 10 replicates. Using the L1000 platform, we varied the detection mode for acquiring the data between two genes per analyte type (DUO) and one gene per analyte type (UNI). We then used the data to run an open innovation competition (as described in @lakhani2013prize and @blasco2019advancing) where we offered cash incentives for different solutions for the deconvolution problem. We assessed the accuracy of these different approaches using the UNI data as the ground truth.

The top approaches included very popular machine-learning methods, such as Random Forests and Convolutional Neural Networks (CNNs), as well as more traditional Expectation-Maximization (EM) approaches based on Gaussian mixtures. Overall, the results of the challenge enabled us to perform a cost-effective comparison of different methods under nearly identical conditions, which we report below. 

<!-- 
Based on this comparison, we found that the competitors' methods achieved significant improvements in accuracy over the k-means benchmark with respect to gene expression values, improved detection of extreme modulations, including an improved ability to detect target knockdowns in shRNA experiments; they yielded more consistent predictions across replicates (a smaller inter-replicate variability), thus leading to more reliable outcomes overall. Additionally, we evaluated the computational speed of all the approaches. Traditional parametric methods, such as the EM, achieved 60x speedup compared to the benchmark, without losing on the accuracy. Machine-learning methods, such as Random Forests, achieved greater accuracy but achieved smaller improvements in terms of speed.
 -->

<!--  of the pair, using additional information about differences in the bead proportions of the genes (within each pair, one gene has a higher proportion of beads than the other). -->

<!-- mean of the largest cluster to the gene in higher proportion and the mean value of the second largest cluster to the gene in lower proportion -->


<!-- 
As a research tool to test for alternative solutions, we used an open innovation competition [@lakhani2013prize; @blasco2019advancing]. In this paper, we report the outcomes of the competition, which successfully engaged a variety of competitorscomputer scientist, software developers and bioinformatics in the problem, resulting in a cost-effective exploration of competing approaches (random forest, gaussian mixture models, and convolutional nets) that would have been otherwise prohibitive.
 -->

<!-- Testing for alternative methods would have required substantial resources to experiment with these alternative approaches (more than what already done) and to adapt new to our data. Moreover, impossible an exhaustive search for all available approaches to try; and the combination of these different approaches.  -->


<!-- 
Shen-orr et al. (2010). Gene expression data from multiple cell-types. The problem is to quantify cell-type frequency "then deconvolve and compare cell type-specific average expression profiles for groups of mixed tissue samples." One approach is "deconvolution to be linear"
 -->

<!-- Broadly speaking, the problem is to estimate the density $f$ of a random variable $X$ based on $n$ observations from $Y = X + \epsilon$, where $\epsilon$ is a measurement error based on a possibly unknown distribution.  (Fan, 1991) -->

<!-- The solution adopted by CMap was a KNN [explain]. This is a standard technique implemented in xxx that gives a good level of accuracy [xxx] and reasonably fast computation time (xxx with the current Matlab implementation). -->




<!-- 
## Related literature 
@musa2017review review of cmap. 
@down2008bayesian A Bayesian deconvolution strategy for immunoprecipitation-based DNA methylome analysis
@liu2015compound used a fuzzy c-means Gaussian Mixture Model (GMM) to process raw L1000 data, showing better performance compared to KNN. This method is described below: 
@zhong2012gene Gene expression deconvolution in linear space
@hunt1971deconvolution one of the first deconv? 
@preibisch2014efficient Efficient Bayesian-based multiview deconvolution
 -->

<!-- 
> To deconvolute such overlapped peaks, we assumed that the fluorophore intensities of each analyte type (corresponding to a specific mRNA type) had a Gaussian distribution. The distribution of the mixture of analytes GeneH(i) and GeneL(i) corresponding to the expression levels of GeneH and GeneL, respectively, should be subject to a bimodal Gaussian distribution, with the proportion of 1.25 to 0.75. We initialized the estimations of the two Gaussian distributions using buzzy c-means clustering [11] and estimated the GMM parameters using the Nelder-Mead method [12]. Thus, the overlapped peaks were deconvoluted as the two estimated Gaussian peaks and the expression levels of the two genes sharing the same analyte were extracted. Mathematical details are included in the Supplementary Methods (the GMM model).
 -->


