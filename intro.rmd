
Introduction
============

<!-- 
A major limitation of multianalytes assays is their limited number of available analyte types. 
In the context of RNA profiling, the CMap group has developed a framework that integrates statistical analysis with traditional multianalyte methods to infer differential expressions for multiple genes from the same analyte type. 
We report the results of an open innovation competition that impvored upon the state-of-the-art by integrating machine-learning techniques to address the deconvultion problem. 
 -->

<!--
Over the last decade, the ability to analyze multiple analytes simultaneously from the same biological sample has contributed to creating large perturbational datasets that are essential to accelerate the discovery of novel therapeutics. However, a fundamental limitation of traditional multianalyte assays lies in the restricted extent and type of available analytes
-->

In biomedical applications, multiplex assays are frequently leveraged to use a single analyte, or sensor, to measure multiple distinct signals (cell types, tissues, and genes). Examples include measuring differential gene expression for genes of specific cell types from mixed samples [@shen2010cell], characterizing single-cell gene-expression profiles in heterogeneous tissues [@deng2019scalable], and understanding cell growth in dynamically evolving populations [@lu2003expression]; but similar situations arise also in many other related contexts, such as imaging [e.g., multiview integration, @preibisch2014efficient]. In these situations, researchers must deconvolute the composite data to identify and isolate measurements from each population.Â 

The scale of biomedical research applications requires that deconvolution algorithms be accurate and computationally efficient, but the development of such algorithms requires appropriate training and validation data. We describe the generation of such a dataset, using the L1000 gene expression assay [@subramanian2017next], and the results of an open innovation competition [@lakhani2013prize; @blasco2019advancing] to improve the deconvolution of L1000 data. The contest drew competitors from across the globe and resulted in a diversity of approaches that significantly improved over the L1000 benchmark and likely have application beyond gene expression.
 
L1000 is a high-throughput assay that lowers (same reproducivility of RNA-seq) by focusing on a reduced representations of gene  expression (focusing on 1000 "landmark" genes from human cells);  measurements are made using the 500 colors of Luminex beads such that two transcripts are identified by a single bead color.  L1000 matches pairs of genes to the same analyte type, or bead, to double the count of profiled genes in a single experiment, thus lowering greatly costs [@subramanian2017next]. A central component of this approach is the computational deconvolution of gene expression signals from multiple genes measured using the same analyte. And has contributed to massive scale-up of Connectivity Map (lamb et al.), from its initial xxx profiles to the current 1.3 million L1000 profiles.  


<!-- Connectopedia: L1000 is a high-throughput gene expression assay that measures the mRNA transcript abundance of 978 "landmark" genes from human cells. (The "L" in L1000 refers to the Landmark genes measured in the assay.) Measurements are made using the 500 colors of Luminex beads such that two transcripts are identified by a single bead color. We also measure expression of 80 control transcripts, chosen for their invariant expression across cell states, as well as the 978 Landmark genes. -->

<!-- within the context of RNA profile, the Connectivity Map (CMap) has developed an assay, called L1000, that integrates traditional multianalyte detection methods and statistical analyses to simultaneously measure the expression of multiple genes using the same analyte type, thus multiplying the capacity of existing technologies [@subramanian2017next]. -->

<!-- 
@cleary2017efficient proposes to use "compressed sensing to use low-level measurements or composite signals (from multiple genes) to recover high-level... 
@deng2019scalable
@hie2019efficient
such as to measure differential gene expression for each gene in mixed cell-type samples [@shen2010cell], 
for studying the state of cells and tissues in response to perturbations
 -->

To measure two different genes by a single bead type, L1000's deconvolution approach, called dpeak, is based on the k-means clustering algorithm. This is a method for cluster analysis, originally from signal processing [@lloyd1982least], that is popular in biology. The approach aims to partition a set of measurements (fluorescence intensity for each bead type) into $k$ clusters by minimizing the within-cluster sum of squares. Differentially expressed genes should have a distribution of measurements with two peaks; and the size of each peak will be a reflection of the proportion of beads used xxxx. By mixing genes in a 2:1 proportion,  the algorithm can tus assigns the clusters to each gene within the sample (see @subramanian2017next for the details). 

The dpeak approach works well in practice but has several limitations; it tends to split clusters incorrectly when the distributions are not reasonably well separated, it is sensitive to outliers, and it is computationally demanding.^[An efficient k-means algorithm, such as Lloyd's, has a running time that scales as $O(kn)$ where $n$ is the number of observations and $k$ the number of clusters.]

Motivated by recent successes with machine learning techniques in biology, we hypothesized better deconvolution approaches that could likely be developed. To test this hypothesis, we generated a novel experimental dataset of the response of approximately 1,000 genes to 122 different perturbagens (shRNA and compounds) within 4 to 10 replicates. Using the L1000 platform, we varied the detection mode for acquiring the data between two genes per analyte type (DUO) and one gene per analyte type (UNI). We then used the data to run an open innovation competition (as described in @lakhani2013prize and @blasco2019advancing) where we offered cash incentives for different solutions for the deconvolution problem. We assessed the accuracy of these different approaches using the UNI data as the ground truth.

The top approaches included very popular machine-learning methods, such as Random Forests and Convolutional Neural Networks (CNNs), as well as more traditional Expectation-Maximization (EM) approaches based on Gaussian mixtures. Overall, the results of the challenge enabled us to perform a cost-effective comparison of different methods under nearly identical conditions, which we report below. 

<!-- 
Based on this comparison, we found that the competitors' methods achieved significant improvements in accuracy over the k-means benchmark with respect to gene expression values, improved detection of extreme modulations, including an improved ability to detect target knockdowns in shRNA experiments; they yielded more consistent predictions across replicates (a smaller inter-replicate variability), thus leading to more reliable outcomes overall. Additionally, we evaluated the computational speed of all the approaches. Traditional parametric methods, such as the EM, achieved 60x speedup compared to the benchmark, without losing on the accuracy. Machine-learning methods, such as Random Forests, achieved greater accuracy but achieved smaller improvements in terms of speed.
 -->

<!--  of the pair, using additional information about differences in the bead proportions of the genes (within each pair, one gene has a higher proportion of beads than the other). -->

<!-- mean of the largest cluster to the gene in higher proportion and the mean value of the second largest cluster to the gene in lower proportion -->


<!-- 
As a research tool to test for alternative solutions, we used an open innovation competition [@lakhani2013prize; @blasco2019advancing]. In this paper, we report the outcomes of the competition, which successfully engaged a variety of competitorscomputer scientist, software developers and bioinformatics in the problem, resulting in a cost-effective exploration of competing approaches (random forest, gaussian mixture models, and convolutional nets) that would have been otherwise prohibitive.
 -->

<!-- Testing for alternative methods would have required substantial resources to experiment with these alternative approaches (more than what already done) and to adapt new to our data. Moreover, impossible an exhaustive search for all available approaches to try; and the combination of these different approaches.  -->


<!-- 
Shen-orr et al. (2010). Gene expression data from multiple cell-types. The problem is to quantify cell-type frequency "then deconvolve and compare cell type-specific average expression profiles for groups of mixed tissue samples." One approach is "deconvolution to be linear"
 -->

<!-- Broadly speaking, the problem is to estimate the density $f$ of a random variable $X$ based on $n$ observations from $Y = X + \epsilon$, where $\epsilon$ is a measurement error based on a possibly unknown distribution.  (Fan, 1991) -->

<!-- The solution adopted by CMap was a KNN [explain]. This is a standard technique implemented in xxx that gives a good level of accuracy [xxx] and reasonably fast computation time (xxx with the current Matlab implementation). -->




<!-- 
## Related literature 
@musa2017review review of cmap. 
@down2008bayesian A Bayesian deconvolution strategy for immunoprecipitation-based DNA methylome analysis
@liu2015compound used a fuzzy c-means Gaussian Mixture Model (GMM) to process raw L1000 data, showing better performance compared to KNN. This method is described below: 
@zhong2012gene Gene expression deconvolution in linear space
@hunt1971deconvolution one of the first deconv? 
@preibisch2014efficient Efficient Bayesian-based multiview deconvolution
 -->

<!-- 
> To deconvolute such overlapped peaks, we assumed that the fluorophore intensities of each analyte type (corresponding to a specific mRNA type) had a Gaussian distribution. The distribution of the mixture of analytes GeneH(i) and GeneL(i) corresponding to the expression levels of GeneH and GeneL, respectively, should be subject to a bimodal Gaussian distribution, with the proportion of 1.25 to 0.75. We initialized the estimations of the two Gaussian distributions using buzzy c-means clustering [11] and estimated the GMM parameters using the Nelder-Mead method [12]. Thus, the overlapped peaks were deconvoluted as the two estimated Gaussian peaks and the expression levels of the two genes sharing the same analyte were extracted. Mathematical details are included in the Supplementary Methods (the GMM model).
 -->


