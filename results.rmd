```{r results-setup, include=FALSE}
options(digits=2)
library(data.table)
library(dplyr, warn=FALSE)
library(knitr)
speed_acc <- fread("data/holdout_score_time.txt", na.strings="n/a")
soln_desc <- fread("data/solution_descriptions.txt", na.strings="n/a")
speed_acc <- merge(speed_acc, soln_desc, by="handle")
```

```{r main-stats, include = FALSE}
percent <- function (x, digits = 0, brackets = FALSE) {
    pc <- paste0(round(x * 100, digits), "%")
    if (brackets) 
        pc <- paste0("(", pc, ")")
    return(pc)
}
bench.auc <- speed_acc$auc[speed_acc$handle == "benchmark"]
bench.cor <- speed_acc$cor[speed_acc$handle == "benchmark"]
bench.sec <- speed_acc$sec[speed_acc$handle == "benchmark"]
plate_id <- as.numeric(factor(speed_acc$plate))

# avg. improvements
ols.sec <- lm((sec - bench.sec[plate_id])/bench.sec[plate_id] ~ 1, data=speed_acc, subset = handle!="benchmark")
ols.auc <- lm(log(auc / bench.auc[plate_id]) ~ 1, data=speed_acc, subset = handle!="benchmark")
ols.cor <- lm(log(cor / bench.cor[plate_id]) ~ 1, data=speed_acc, subset = handle!="benchmark")
ci <- sapply(list(sec=ols.sec,auc=ols.auc,cor=ols.cor), confint)
speed_acc$sec / bench.sec[plate_id] 
``` 

# Results

## Participation

The contest attracted 294 participants, who made 820 code submissions with an average of about 18 submissions per participant. The top finishers in the contest employed a variety of different analysis approaches, including descision tree regressors (DTR), Gaussian mixture models (GMM), convolutional neural networks (CNN), and customized versions of k-means, all with notably improved performance relative to the benchmark. Table \ref{tab:soln_desc} lists the top 9 finishers and the languages and algorithms each used.

```{r soln_desc, echo=F, warning=F}
# display table of solution description
soln_desc <- fread("data/solution_descriptions.txt", na.strings = "n/a")
setnames(soln_desc, "method_short", "category")
kable(soln_desc[1:9], caption = "\\label{tab:soln_desc}Summary of contestant solutions")
```

<!--  Fig. 1. Participation stats (Submission counts) -->

## Overall accuracy and speed

We tested the accuracy and speed of the competitors' solutions on the holdout L1000 data obtained by applying the DUO detection method (two genes per analyte color) and utilizing as the ground truth the data obtained by employing the UNI detection method (one gene per analyte color).

We computed the gene-level rank correlation with the ground truth for each of the top nine algorithms and the benchmark. 
Overall, the correlation with the ground truth was high for all the solutions (r = xxxx). All the competitors' solutions improved in this metric over the k-means benchmark (Fig, XXXX), although the observed improvements were modest (2-3 percent).

Next, we tested differences in the probability of detection of extreme modulated genes (genes with abs higher than xxx). 
All algorithms achieved a good detection performance (AUC = xxx). Also in this case, we observe improvements, although modest, from all the competitors' solutions (between xxx and xxx percent). 
<!-- The benchmark achieved a good performance on accuracy (an AUC of `r bench.auc[1]` and `r bench.auc[2]` and a correlation of `r bench.cor[1]` and `r bench.cor[2]` for plate 1 and 2, respectively) with a corresponding runtime of about 4 minutes per plate (`r bench.sec[1]` and `r bench.sec[2]` seconds). All the contestants achieved improvements in the rank correlation over the benchmark mwere modest `r percent(coef(ols.cor), digits=1)`  (95% CI, `r percent(ci[1, "cor"],1)` to `r percent(ci[2, "cor"],1)`) -->

We further tested differences in the probability oof detection of the extreme modulations for knockdown genes (explain). For this comparison, we computed the metric also for the UNI data, and compared performance of... 
We observe a high  performance in accuracy. xxx we computed the KD success frequency for each algorithm and the benchmark. This metric is the fraction of the 376 landmark-targeting shRNA experiments in the holdout data for which the predictions met these success criteria. We used the $UNI$ ground-truth data to estimate the maximum achievable KD success frequency, which in this case was 0.8. We observe that all but 2 of the top 9 contestant algorithms achieve a higher KD success frequency than the benchmark solution (\Fig{kd_barplot}). These results suggest that the algorithm improvements, as assessed by the accuracy metrics used in the contest, translate to improvements in biologically relevent metrics used in common applications of L1000 data.

Finally, speed improvements over the k-means benchmark were substantial (\Fig{acc-vs-speed}). The fastest algorithm took as little as `r min(speed_acc$sec)` seconds per plate (xxx% improvment), and the slowest took xxx seconds (xxx% improvement). 

<!-- 
Also, we observe no significant difference in performance between the plates, with all the submissions achieving similar scores in both plates; and no accuracy-vs-speed trade-off, as the empirical relationship between accuracy and speed improvements is essentially flat. 
 -->

<!-- Note that the theoretical maximum improvement in the AUC metric was 14% = 1/bench, and the theoretical maximum improvement for the correlation was 60% = 1/cor.bench, given both measures cannot be greater than one. Thus, we find solutions achieved greater improvements in auc relative to the theoretical max, than in the correlation metric.   -->

<!-- 
Only two submissions (out of nine) a performance that was better in AUC and correlation but worse than the benchmark on the KD success metric. These submissions, both based on k-means, ranked sixth and eighth in the final leaderboard.
 -->

Further examination based on the underlying methods used by the top solutions suggests that parametric models, such as mixture models, are both fast and relatively accurate, although non-parametric models, such as regression forests and k-means, tend to be more accurate.  The fastest algorithm was indeed based on a parametric model, i.e., gaussian mixture model, which achieves a good level of accuracy on these data. This solution ranked second overall and achieved the greatest speed improvmenet in both plates. The most accurate algorithm was instead based on a random forest. This solution ranked first overall and achieved the best performance in both AUC and correlation measures, as well as it was among the top three approaches in the KD measure. 


\color{red}

Todos:
- Update equations to use Latex

\color{black}


### Reduction variation across replicate samples

One of the issues with the benchmark k-means solution is that it does little to mitigate the discrepancy in prediction accuracy between the genes measued with high and low bead proportions. 

\color{red}

Todos:
- boxplots of replicate variance, stratified by algo and hi/lo bead
- barplot of average variance per algo, showing that winning algo has lowest variance and discrepancy b/w hi/lo bead is minimized

\color{black}

## Clustering submissions

### 2D projection

Given the variety of methods represented amongst the prize-winning solutions, we sought to assess whether there were notable differences in the predictions generated. Using the holdout dataset, we generated a two-dimensional projection of the $UNI$ ground truth data and $DUO$-derived benchmark and contenstant predictions for both $DECONV$ and $DE$ data using t-distributed stochastic neighbor embedding (t-SNE, van der Maaten et al [ref]). Each point represents a single ground truth or predicted sample.

We observe that in $DECONV$ data the samples primarily cluster by pertubagen type, with the exeption of the ground truth $UNI$ data, which appears distinct from the deconvoluted samples (\Fig{tsne_figure} A and B). Separating the samples by algorithm reveals commonalities in the predictions generated by similar algorithms (\Fig{tsne_figure} C). For example, the decision tree regressor (DTR) algorithms have similar 'footprints' in the projection, as do the k-means and Gaussian mixture model (GMM) algorithms. This suggests that in general similar algorithms generate predictions with similar properties.

After the standard transformation to $DE$ data we observe that the t-SNE projection is much more homogenous, indicating that perturbagen type and algorithm-specific affects have been greatly reduced (\Fig{tsne_figure} D). This is reassuring, given that in production mode downstream analysis of this data will be based on $DE$. It also suggests that integrating multiple deconvolution algorithms into an ensemble method might be feasible. 

### Clustering by gene-Level performance

We observed that the global structure of DE data seemed independent of algorithm type in general. We additionally sought to understand whether there were differences between the algorithms at the individual gene level. To assess this, we identified the best performing algorithm (by correlation metric) for each of the 976 landmark genes. We observe that while the contest winner is the best performer for the majority of genes, over 70% of the genes achieve better correlation with a different algorithm, and all but 2 algorithms are the best performers for at least 5% of the genes. This suggest that there may be complementarity between the algorithms, which could potentially be leveraged by an ensemble approach.

## Ensemble approaches

Figure 3. (A) Scatterplot runtime vs accuracy for ensamble (slides p. 163)

Speed vs accuarcy trade-off. Integration one or multiple methods?


## Minors:

- signs of overfitting (compare traing vs testing)


