# Results

## Participation

The contest attracted 294 participants, who made 820 code submissions, an averge of 18.2 submissions per participant. The top finishers in the contest employed a variety of different analysis approaches, including descision tree regressors (DTR), Gaussian mixture models (GMM), convolutional neural networks (CNN), and customized versions of k-means, all with notably improved performance relative to the benchmark. Table xx lists the top 9 finishers and the languages and algorithms each used.

```{r soln_desc, echo=F, warning=F}
# display table of solution description
library(data.table)
library(knitr)
soln_desc <- fread("data/solution_descriptions.txt", na.strings = "n/a")
setnames(soln_desc, "method_short", "category")
kable(soln_desc[1:9], caption = "Summary of contestant solutions")
```

Fig. 1. Participation stats (Submission counts)

## Overall accuracy and speed

Using the holdout dataset, we computed the average AUC (on level-four data), the average rank correlation (level-two data), as well as the fraction of knocked-down genes correctly predicted for each of the top solutions and the benchmark. The benchmark achieved an AUC of xxx and xxx, a correlation of xxx and xxx, and successfully predicted xxx and xxx knocked-down genes for plate 1 and 2, respectively. The corresponding runtime for the benchmark in each plate was xxx and xxx seconds. 

\Fig{acc-vs-speed} shows that almost all top-nine solutions improved upon the benchmark along these measures. Improvements varied across the different measurs. Overall, the average AUC improvement was 2%, the average correlation improvement was 3% and the average KD improvement was 3%. Note that the theoretical maximum improvement in the AUC metric was 14% = 1/bench, and the theoretical maximum improvement for the correlation was 60% = 1/cor.bench, given both measures cannot be greater than one. Thus, we find solutions achieved greater improvements in auc relative to the theoretical max, than in the correlation metric.  

Note further that, while all submissions improved upon the benchmark in auc and correlation, two out of nine submissions achieved a performance that was below the benchmark on the KD success metric. These submissions, both based on k-means, ranked sixth and eighth in the final leaderboard.

The top competitors achieved  very significant 300% average improvement in speed, that varied widely among solutions ranging between about 100% to 500%.  

We also observe a relatively flat relationship between improvements in accuracy and improvements in speed, with no particular trade-off in the top solutions. The fastest algorithm, ranked second overall, was based on a gaussian mixture model and achieved the greatest speed improvmenet in both plates. The most accurate algorithm, ranked first overall, was instead based on a random forest and achieved the best performance in both AUC and correlation measures, and it was among the top three approaches in the KD measure. 

We also observe no significant difference in performance between the plates, with all the submissions achieving similar scores in both plates. 

<!-- 
Fig. 2. (A) Leaderboard (all scores) 
        (B) Disaggregated scores for top 10 
            (barplot for mean of the 2 plates by submission and for each metric)
        (C) Scatter plot runtime vs accuracy (mean of AUC and Correlation)
 -->

\color{red}

Todos:

- In Methods section, explain accuracy as measured in the coontest (slide p. 124). And then explain, KD additional test of accuracy (slides p. 128). Results are good on both.
- (How far froom the max achievable improvement in accuracy (down-sampling uni)?)
- Discrepancy between genes with high/low bead counts.

\color{black}

## Clustering Submissions

Given the variety of methods represented amongst the prize-winning solutions, we sought to assess whether there were notable differences in the predictions generated. Using the holdout dataset, we generated a two-dimensional projection of the $UNI$ ground truth data and $DUO$-derived benchmark and contenstant predictions for both $DECONV$ and $DE$ data using t-distributed stochastic neighbor embedding (t-SNE, van der Maaten et al [ref]). Each point represents a single ground truth or predicted sample.

We observe that in $DECONV$ data the samples primarily cluster by pertubagen type, with the exeption of the ground truth $UNI$ data, which appears distinct from the deconvoluted samples (\Fig{tsne_figure} A and B). Separating the samples by algorithm reveals commonalities in the predictions generated by similar algorithms (\Fig{tsne_figure} C). For example, the decision tree regressor (DTR) algorithms have similar 'footprints' in the projection, as do the k-means and Gaussian mixture model (GMM) algorithms. This suggests that in general similar algorithms generate predictions with similar properties.

After the standard transformation to $DE$ data we observe that the t-SNE projection is much more homogenous, indicating that perturbagen type and algorithm-specific affects have been greatly reduced (\Fig{tsne_figure} D). This is reassuring, given that in production mode downstream analysis of this data will be based on $DE$. It also suggests that integrating multiple deconvolution algorithms into an ensemble method might be feasible. 

## Ensambles.

Figure 3. (A) Scatterplot runtime vs accuracy for ensamble (slides p. 163)

Speed vs accuarcy trade-off. Integration one or multiple methods?


## Minors:

- signs of ovverfitting (compare traing vs testing)


