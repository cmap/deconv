# Results

## Participation

The contest attracted 294 participants, who made 820 code submissions, an averge of 18.2 submissions per participant. The top finishers in the contest employed a variety of different analysis approaches, including descision tree regressors (DTR), Gaussian mixture models (GMM), convolutional neural networks (CNN), and customized versions of k-means, all with notably improved performance relative to the benchmark. Table xx lists the top 9 finishers and the languages and algorithms each used.

```{r soln_desc, echo=F, warning=F}
# display table of solution description
library(data.table)
library(knitr)
soln_desc <- fread("data/solution_descriptions.txt", na.strings = "n/a")
setnames(soln_desc, "method_short", "category")
kable(soln_desc[1:9], caption = "Summary of contestant solutions")
```

Fig. 1. Participation stats (Submission counts)

## Overall accuracy and speed

Using the holdout dataset, we computed the average AUC (on level-four data), the average rank correlation (level-two data), as well as the fraction of knocked-down genes correctly predicted for each of the top solutions and the benchmark. The benchmark achieved an AUC of xxx and xxx, a correlation of xxx and xxx for plate 1 and 2, respectively, as well as  it successfully predicted xxx knocked-down genes in plate 2. The corresponding runtime for the benchmark in each plate was of xxx and xxx seconds. 

\Fig{acc-vs-speed} shows that all top-nine solutions improved upon the performance of the benchmark in nearly all measures. Overall, the average AUC improvement was of 2% (min = xxx, max =xxxx), the average correlation improvement was of 3% (min = xxx, max = xxx) and the average improvement in correct kd genes predictions was of 3% (min = xxx, max =xxx).  Given a theoretical maximum improvement of xxx, xxx, and xxx, for the AUC, correlation and KD metrics, we observe that competitors achieves larger improvements (relative to the maximum achievable) in the AUC metric. 

Only two submissions (out of nine) a performance that was better in AUC and correlation but worse than the benchmark on the KD success metric. These submissions, both based on k-means, ranked sixth and eighth in the final leaderboard.


<!-- Note that the theoretical maximum improvement in the AUC metric was 14% = 1/bench, and the theoretical maximum improvement for the correlation was 60% = 1/cor.bench, given both measures cannot be greater than one. Thus, we find solutions achieved greater improvements in auc relative to the theoretical max, than in the correlation metric.   -->

Improvement with respect to speed were substantial. The average improvement was 300% ranging between 100% and 500%. We observe no accuracy-vs-speed trade-off. The empirical relationship between improvements in accuracy and improvements in speed is essentially flat. 

Further examination based on the underlying methods used by the top solutions suggests that parametric models, such as mixture models, are both fast and relatively accurate, although non-parametric models, such as regression forests and k-means, tend to be more accurate.  The fastest algorithm was indeed based on a parametric model, i.e., gaussian mixture model, which achieves a good level of accuracy on these data. This solution ranked second overall and achieved the greatest speed improvmenet in both plates. 

The most accurate algorithm was instead based on a random forest. This solution ranked first overall and achieved the best performance in both AUC and correlation measures, as well as it was among the top three approaches in the KD measure. 

We observe no significant difference in performance between the plates, with all the submissions achieving similar scores in both plates. 

<!-- 
Fig. 2. (A) Leaderboard (all scores) 
        (B) Disaggregated scores for top 10 
            (barplot for mean of the 2 plates by submission and for each metric)
        (C) Scatter plot runtime vs accuracy (mean of AUC and Correlation)
 -->

\color{red}

Todos:

- In Methods section, explain accuracy as measured in the coontest (slide p. 124). And then explain, KD additional test of accuracy (slides p. 128). Results are good on both.
- (How far froom the max achievable improvement in accuracy (down-sampling uni)?)
- Discrepancy between genes with high/low bead counts.
- Update equations to use Latex

\color{black}

### Knockdown predictions

In addition to the Spearman correlation and extreme modulation AUC metrics used in the contest, we were also able to asses each algorithms ability to correctly predict the successful knockdown (KD) of landmark genes. The shRNA experiments used to generate the contest data were constructed such that each shRNA specifically targeted one of the landmark genes. Hence, the expectation in each of those experiments is that the targeted landmark gene should exhibit a greatly reduced expression level, which should manifest in a very low z-score in $DE$ data. Additionally, we expect that the targeted landmark gene should be most dramatically down-regulated in the samples in which it was directly targeted. To assess this, we compute a gene-wise rank by $DE$ z-score across all samples in a given plate. Criteria for indicating a successful KD are $zs <= -2 AND rank <= -10$. For each algorith, we compouted the KD success frequency as the fraction of the 376 landmark-targeting shRNA experiments in the holdout data for which the predictions met these success criteria. We used the $UNI$ ground-truth data to estimate the maximum achievable KD success frequency, which in this case was 0.8. We observe that all but 2 of the top 9 contestant algorithms achieve a higher KD success frequency than the benchmark solution (\Fig{kd_barplot}). These results suggest that the algorithm improvements, as assessed by the accuracy metrics used in the contest, translate to improvements in biologically relevent metrics used in common applications of L1000 data.

\color{red}
TODO: 1) compute precision/recall for KD predictions

\color{black}

### Reduction of difference between low and high bead proportions

One of the issues with the benchmark k-means solution is that it does little to mitigate the discrepancy in prediction accuracy between the genes measued with high and low bead proportions. We observe that the contestant solutions reduce this difference, as measured 

\color{red}
TODO: 1) global distributions of DECONV values split by hi/low prop
      2) include UNI for comparison
      3) show that variance is higher for low prop but difference is reduced w/ new algos

\color{black}

## Clustering submissions

### 2D projection

Given the variety of methods represented amongst the prize-winning solutions, we sought to assess whether there were notable differences in the predictions generated. Using the holdout dataset, we generated a two-dimensional projection of the $UNI$ ground truth data and $DUO$-derived benchmark and contenstant predictions for both $DECONV$ and $DE$ data using t-distributed stochastic neighbor embedding (t-SNE, van der Maaten et al [ref]). Each point represents a single ground truth or predicted sample.

We observe that in $DECONV$ data the samples primarily cluster by pertubagen type, with the exeption of the ground truth $UNI$ data, which appears distinct from the deconvoluted samples (\Fig{tsne_figure} A and B). Separating the samples by algorithm reveals commonalities in the predictions generated by similar algorithms (\Fig{tsne_figure} C). For example, the decision tree regressor (DTR) algorithms have similar 'footprints' in the projection, as do the k-means and Gaussian mixture model (GMM) algorithms. This suggests that in general similar algorithms generate predictions with similar properties.

After the standard transformation to $DE$ data we observe that the t-SNE projection is much more homogenous, indicating that perturbagen type and algorithm-specific affects have been greatly reduced (\Fig{tsne_figure} D). This is reassuring, given that in production mode downstream analysis of this data will be based on $DE$. It also suggests that integrating multiple deconvolution algorithms into an ensemble method might be feasible. 

### Clustering by gene-Level performance

We observed that the global structure of DE data seemed independent of algorithm type in general. We additionally sought to understand whether there were differences between the algorithms at the individual gene level. To assess this, we clustered the algorithms in the space of their gene-level accuracy metrics (Figure XX). We observe that for a large proportion of genes, all algorithms perform about equally well. There are subsets of genes for which accuracy is a bit more algorithm-dependent \color{red}(ex. foo bar).\color{black} These differences suggest complementarity between the algorithms which could potentially be leveraged using an ensemble approach.

\color{red}
It seems that the gene-level performance for the top algorithms is quite similar. There are very few for which one algorithm is notably different than the rest. This does not immediately suggest obvious complementarity between the algorithms, but we can formally test this by aggregating the predictions from all possible algorithm combinations and assessing their accuracy.
\color{black}

## Ensemble approaches

Figure 3. (A) Scatterplot runtime vs accuracy for ensamble (slides p. 163)

Speed vs accuarcy trade-off. Integration one or multiple methods?


## Minors:

- signs of overfitting (compare traing vs testing)


