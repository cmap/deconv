
```{r results-setup, include=FALSE}
options(digits=2)
library(data.table)
library(dplyr, warn=FALSE)
library(knitr)
```

```{r datasets, include = FALSE}
speed_acc <- fread("data/holdout_score_time.txt", na.strings="n/a")
soln_desc <- fread("data/solution_descriptions.txt", na.strings="n/a")
speed_acc <- merge(speed_acc, soln_desc, by="handle")
```

```{r main-stats, include = FALSE}
percent <- function (x, digits = 0, brackets = FALSE) {
    pc <- paste0(round(x * 100, digits), "%")
    if (brackets) 
        pc <- paste0("(", pc, ")")
    return(pc)
}
bench.auc <- speed_acc$auc[speed_acc$handle == "benchmark"]
bench.cor <- speed_acc$cor[speed_acc$handle == "benchmark"]
bench.sec <- speed_acc$sec[speed_acc$handle == "benchmark"]
plate_id <- as.numeric(factor(speed_acc$plate))

# avg. improvements
ols.sec <- lm((sec - bench.sec[plate_id])/bench.sec[plate_id] ~ 1, data=speed_acc, subset = handle!="benchmark")
ols.auc <- lm(log(auc / bench.auc[plate_id]) ~ 1, data=speed_acc, subset = handle!="benchmark")
ols.cor <- lm(log(cor / bench.cor[plate_id]) ~ 1, data=speed_acc, subset = handle!="benchmark")
ci <- sapply(list(sec=ols.sec,auc=ols.auc,cor=ols.cor), confint)
speed_acc$sec / bench.sec[plate_id] 
``` 

# Results

## Participation

The contest attracted 294 participants, who made 820 code submissions with an average of about 18 submissions per participant. The top finishers in the contest employed a variety of different analysis approaches, including descision tree regressors (DTR), Gaussian mixture models (GMM), convolutional neural networks (CNN), and customized versions of k-means, all with notably improved performance relative to the benchmark. Table \ref{tab:soln_desc} lists the top 9 finishers and the languages and algorithms each used.

```{r soln_desc, echo=F, warning=F}
soln_desc <- fread("data/solution_descriptions.txt", na.strings = "n/a")
setnames(soln_desc, "method_short", "category")
kable(soln_desc[1:9], caption = "\\label{tab:soln_desc}Summary of contestant solutions")
```

<!--  Fig. 1. Participation stats (Submission counts) -->

## Accuracy and speed

We tested the accuracy and speed of the competitors' solutions on the holdout L1000 data obtained by applying the DUO detection method (two genes per bead color) and utilizing the data obtained by employing the UNI detection method (one gene per bead color) as the ground truth.

*Correlation.*  The Spearman rank correlations between gene-level expressions of the ground truth and those of each of the top nine algorithms and the benchmark were high overall ($\rho$ > `r min(speed_acc$cor)`). For both shRNA and compound experiments, the empirical distribution of the solutions made by the competitors was right-shifted compared to the k-means benchmark, indicating more accurate predictions (\Fig{accuracy-fig}, A and B); the median Spearman rank correlation of the competitors was significantly higher than the equivalent for the benchmark (\Fig{accuracy-fig}, C and D), although the size of the observed improvements was modest (2-3 percentage points).

Next, we counted the number of genes with the highest Spearman rank correlation for each algorithm. Results (\Fig{xxx}) showed that the winning algorithm, based on a random forest, accounted for about one third (33%) of the genes; the second placed algorithm (based on EM) accounted for one fifth (20%); and the fourth placed algorithm (CNN) accounted for another approximately one fifth (20%). Thus, these three approaches achieved jointly the best performance for more than 70% of the genes in our samples.

*Extreme modulations.* We further tested the accuracy of the competitors' solutions on the detection of, so called, “extreme modulations.”  These are genes that are notably up- or down-regulated by perturbation, relative to the absence of perturbagens treatment and, hence, exhibit an exceedingly high (or low) differential expression (DE) values. 
We obtained the DE values by using a robust z-score procedure (as described by @subramanian2017next) and evaluated the detection accuracy of each solution by computing the corresponding area under the curve (AUC); all the solutions achieved a good detection accuracy (AUC > `r  min(speed_acc$auc)`). Again, the competitors' solutions achieved signicant improvements relative to the benchmark (\Fig{auc}).

*Clustering.* Given the variety of methods represented amongst the prize-winning solutions, we sought to assess whether there were notable differences in the predictions generated. Using the holdout dataset, we generated a two-dimensional projection of the $UNI$ ground truth data and $DUO$-derived benchmark and contenstant predictions for both $DECONV$ and $DE$ data using t-distributed stochastic neighbor embedding (t-SNE, @maaten2008visualizing). We observe that in $DECONV$ data the samples primarily cluster by pertubagen type, with the exeption of the ground truth $UNI$ data, which appears distinct from the deconvoluted samples (\Fig{tsne_figure} A and B). Separating the samples by algorithm reveals commonalities in the predictions generated by similar algorithms (\Fig{tsne_figure} C). For example, the decision tree regressor (DTR) algorithms have similar 'footprints' in the projection, as do the k-means and Gaussian mixture model (GMM) algorithms. This suggests that in general similar algorithms generate predictions with similar properties. After the standard transformation to $DE$ data we observe that the t-SNE projection is much more homogenous, indicating that perturbagen type and algorithm-specific affects have been greatly reduced (\Fig{tsne_figure} D). This is reassuring, given that in production mode downstream analysis of this data will be based on $DE$. It also suggests that integrating multiple deconvolution algorithms into an ensemble method might be feasible. 

*Gene knockdowns.* We further tested differences in the probability oof detection of the extreme modulations for knockdown genes (explain). For this comparison, we computed the metric also for the UNI data, and compared performance of...  We observe a high  performance in accuracy. xxx we computed the KD success frequency for each algorithm and the benchmark. This metric is the fraction of the 376 landmark-targeting shRNA experiments in the holdout data for which the predictions met these success criteria. We used the $UNI$ ground-truth data to estimate the maximum achievable KD success frequency, which in this case was 0.8. We observe that all but 2 of the top 9 contestant algorithms achieve a higher KD success frequency than the benchmark solution (\Fig{kd}). These results suggest that the algorithm improvements, as assessed by the accuracy metrics used in the contest, translate to improvements in biologically relevent metrics used in common applications of L1000 data.

*Inter-replicate variance.* Agreement between experimental replicates is crucial xxx [ref] 
"The inter-replicate variation of gene expression-quantities is of the utmost importance to biologists because lower variance means higher reproducibility." [refXXX]. Our data contain several replicates which enabled us to study improvements in inter-replicability variance.   See figure \Fig{inter-replicate-variance}. 

*Speed.* Speed improvements over the benchmark were substantial (\Fig{speed}). The benchmark took about 4 minutes per plates. In contrast, the fastest algorithm took as little as `r min(speed_acc$sec)` seconds per plate (a 60x speedup compared to the benchmark), and the slowest was well below one minute per plate. We observed no particular trade-off between speed and accuracy. The fastest algorithm ("ardavel"), that was based a gaussian mixture model, achieved a good level of accuracy as well, and ranked second overall. On the other hand, the algorithm with the best performance in terms of accuracy ("gardn999"), which was based on a decision tree regression, also achieved a decent speed performance compared to the benchmark. Thus, at least within the context of the implemented solutions, we found a negligible trade-off between speed and accuracy.

<!-- 
Also, we observe no significant difference in performance between the plates, with all the submissions achieving similar scores in both plates; and no accuracy-vs-speed trade-off, as the empirical relationship between accuracy and speed improvements is essentially flat. 
 -->

<!-- Note that the theoretical maximum improvement in the AUC metric was 14% = 1/bench, and the theoretical maximum improvement for the correlation was 60% = 1/cor.bench, given both measures cannot be greater than one. Thus, we find solutions achieved greater improvements in auc relative to the theoretical max, than in the correlation metric.   -->

<!-- 
Only two submissions (out of nine) a performance that was better in AUC and correlation but worse than the benchmark on the KD success metric. These submissions, both based on k-means, ranked sixth and eighth in the final leaderboard.
 -->

<!-- Further examination based on the underlying methods used by the top solutions suggests that parametric models, such as mixture models, are both fast and relatively accurate, although non-parametric models, such as regression forests and k-means, tend to be more accurate.  The fastest algorithm was indeed based on a parametric model, i.e., gaussian mixture model, which achieves a good level of accuracy on these data. This solution ranked second overall and achieved the greatest speed improvmenet in both plates. The most accurate algorithm was instead based on a random forest. This solution ranked first overall and achieved the best performance in both AUC and correlation measures, as well as it was among the top three approaches in the KD measure.  -->

### Reduction variation across replicate samples

One of the issues with the benchmark k-means solution is that it does little to mitigate the discrepancy in prediction accuracy between the genes measued with high and low bead proportions. 

\color{red}

Todos:
- boxplots of replicate variance, stratified by algo and hi/lo bead
- barplot of average variance per algo, showing that winning algo has lowest variance and discrepancy b/w hi/lo bead is minimized

\color{black}

### Clustering by gene-Level performance

We observed that the global structure of DE data seemed independent of algorithm type in general. We additionally sought to understand whether there were differences between the algorithms at the individual gene level. To assess this, we identified the best performing algorithm (by correlation metric) for each of the 976 landmark genes. We observe that while the contest winner is the best performer for the majority of genes, over 70% of the genes achieve better correlation with a different algorithm, and all but 2 algorithms are the best performers for at least 5% of the genes. This suggest that there may be complementarity between the algorithms, which could potentially be leveraged by an ensemble approach.

## Ensemble approaches

Figure 3. (A) Scatterplot runtime vs accuracy for ensamble (slides p. 163)

Speed vs accuarcy trade-off. Integration one or multiple methods?


## Minors:

- signs of overfitting (compare traing vs testing)


