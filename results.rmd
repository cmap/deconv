# Results

#### Participation

The contest attracted xxx participants, who made xxx code submissions (a median of xxx per person). 

Fig. 1. Participation stats (Submission counts)

#### Overall accuracy and speed.

Fig. 2. (A) Leaderboard (all scores) 
        (B) Disaggregated scores for top 10 
            (barplot for mean of the 2 plates by submission and for each metric)
        (C) Scatter plot runtime vs accuracy (mean of AUC and Correlation)


```{r, echo=F, warning=F, }
# display table of solution description
library(data.table)
library(knitr)
soln_desc <- fread("data/solution_descriptions.txt", na.strings = "n/a")
kable(soln_desc[1:9], caption = "Summary of contestant solutions")
```


Explain accuracy as measured in the coontest (slide p. 124). And then explain, KD additional test of accuracy (slides p. 128). Results are good on both.

(How far froom the max achievable improvement in accuracy (down-sampling uni)?)

Discrepancy between genes with high/low bead counts.

##### Clustering Submissions.

Do methods overlap? Not at a level that we care about.

Figure 3. (A) Clustering by genes (high ovverlap); 
          (B) TS1-2 Seem to be clustering by method
          (C) Differences mitigated after standard normalization procedure

#### Ensambles.

Figure 3. (A) Scatterplot runtime vs accuracy for ensamble (slides p. 163)

Speed vs accuarcy trade-off. Integration one or multiple methods?


#### Minors:

- signs of ovverfitting (compare traing vs testing)


