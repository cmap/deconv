```{r results-setup, include=FALSE}
options(digits=2)
library(data.table)
library(dplyr, warn=FALSE)
library(knitr)
speed_acc <- fread("data/holdout_score_time.txt", na.strings="n/a")
soln_desc <- fread("data/solution_descriptions.txt", na.strings="n/a")
speed_acc <- merge(speed_acc, soln_desc, by="handle")
```

```{r main-stats, include = FALSE}
percent <- function (x, digits = 0, brackets = FALSE) {
    pc <- paste0(round(x * 100, digits), "%")
    if (brackets) 
        pc <- paste0("(", pc, ")")
    return(pc)
}
bench.auc <- speed_acc$auc[speed_acc$handle == "benchmark"]
bench.cor <- speed_acc$cor[speed_acc$handle == "benchmark"]
bench.sec <- speed_acc$sec[speed_acc$handle == "benchmark"]
plate_id <- as.numeric(factor(speed_acc$plate))

# avg. improvements
ols.sec <- lm((sec - bench.sec[plate_id])/bench.sec[plate_id] ~ 1, data=speed_acc, subset = handle!="benchmark")
ols.auc <- lm(log(auc / bench.auc[plate_id]) ~ 1, data=speed_acc, subset = handle!="benchmark")
ols.cor <- lm(log(cor / bench.cor[plate_id]) ~ 1, data=speed_acc, subset = handle!="benchmark")
ci <- sapply(list(sec=ols.sec,auc=ols.auc,cor=ols.cor), confint)

speed_acc$sec / bench.sec[plate_id] 
``` 

# Results

## Participation

The contest attracted 294 participants, who made 820 code submissions with an average of about 18 submissions per participant. The top finishers in the contest employed a variety of different analysis approaches, including descision tree regressors (DTR), Gaussian mixture models (GMM), convolutional neural networks (CNN), and customized versions of k-means, all with notably improved performance relative to the benchmark. Table \ref{tab:soln_desc} lists the top 9 finishers and the languages and algorithms each used.

```{r soln_desc, echo=F, warning=F}
# display table of solution description
soln_desc <- fread("data/solution_descriptions.txt", na.strings = "n/a")
setnames(soln_desc, "method_short", "category")
kable(soln_desc[1:9], caption = "\\label{tab:soln_desc}Summary of contestant solutions")
```

<!--  Fig. 1. Participation stats (Submission counts) -->

## Overall accuracy and speed

Using the holdout (DE and DECONV) datasets obtained by the UNI and DUO detection methods, we computed the average gene-level AUC (DE data) and rank correlation (DECONV data) for the top nine algorithms and the benchmark. The benchmark achieved a good performance on accuracy (an AUC of `r bench.auc[1]` and `r bench.auc[2]` and a correlation of `r bench.cor[1]` and `r bench.cor[2]` for plate 1 and 2, respectively) with a corresponding runtime of about 4 minutes per plate (`r bench.sec[1]` and `r bench.sec[2]` seconds).

Although all top-nine solutions achieved a higher performance in both accuracy metrics compared to the benchmark, improvements with respect to accuracy were relatively small (\Fig{acc-vs-speed}). The mean AUC improvement was of `r percent(coef(ols.auc), digits=1)` (95% CI, `r percent(ci[1, "auc"], 1)` to `r percent(ci[2, "auc"], 1)`) and the mean correlation improvement was of `r percent(coef(ols.cor), digits=1)`  (95% CI, `r percent(ci[1, "cor"],1)` to `r percent(ci[2, "cor"],1)`). In contrast, improvements with respect to speed were substantial (\Fig{acc-vs-speed}) with the fastest algorithm taking as little as `r min(speed_acc$sec)` seconds and all the submissions achieving a mean runtime reduction of `r percent(abs(coef(ols.sec)))` per plate.  Also, we observe no significant difference in performance between the plates, with all the submissions achieving similar scores in both plates; and no accuracy-vs-speed trade-off, as the empirical relationship between accuracy and speed improvements is essentially flat. 

<!-- Note that the theoretical maximum improvement in the AUC metric was 14% = 1/bench, and the theoretical maximum improvement for the correlation was 60% = 1/cor.bench, given both measures cannot be greater than one. Thus, we find solutions achieved greater improvements in auc relative to the theoretical max, than in the correlation metric.   -->

<!-- 
Only two submissions (out of nine) a performance that was better in AUC and correlation but worse than the benchmark on the KD success metric. These submissions, both based on k-means, ranked sixth and eighth in the final leaderboard.
 -->

Further examination based on the underlying methods used by the top solutions suggests that parametric models, such as mixture models, are both fast and relatively accurate, although non-parametric models, such as regression forests and k-means, tend to be more accurate.  The fastest algorithm was indeed based on a parametric model, i.e., gaussian mixture model, which achieves a good level of accuracy on these data. This solution ranked second overall and achieved the greatest speed improvmenet in both plates. The most accurate algorithm was instead based on a random forest. This solution ranked first overall and achieved the best performance in both AUC and correlation measures, as well as it was among the top three approaches in the KD measure. 

To further test differences in accuracy performance we computed the KD success frequency for each algorithm and the benchmark. This metric is the fraction of the 376 landmark-targeting shRNA experiments in the holdout data for which the predictions met these success criteria. We used the $UNI$ ground-truth data to estimate the maximum achievable KD success frequency, which in this case was 0.8. We observe that all but 2 of the top 9 contestant algorithms achieve a higher KD success frequency than the benchmark solution (\Fig{kd_barplot}). These results suggest that the algorithm improvements, as assessed by the accuracy metrics used in the contest, translate to improvements in biologically relevent metrics used in common applications of L1000 data.

\color{red}

Todos:

- In Methods section, explain accuracy as measured in the coontest (slide p. 124). And then explain, KD additional test of accuracy (slides p. 128). Results are good on both.
- (How far froom the max achievable improvement in accuracy (down-sampling uni)?)
- Discrepancy between genes with high/low bead counts.
- Update equations to use Latex

\color{black}


\color{red}
TODO: 1) compute precision/recall for KD predictions

\color{black}

### Reduction of difference between low and high bead proportions

One of the issues with the benchmark k-means solution is that it does little to mitigate the discrepancy in prediction accuracy between the genes measued with high and low bead proportions. We observe that the contestant solutions reduce this difference, as measured 

\color{red}
TODO: 1) global distributions of DECONV values split by hi/low prop
      2) include UNI for comparison
      3) show that variance is higher for low prop but difference is reduced w/ new algos

\color{black}

## Clustering submissions

### 2D projection

Given the variety of methods represented amongst the prize-winning solutions, we sought to assess whether there were notable differences in the predictions generated. Using the holdout dataset, we generated a two-dimensional projection of the $UNI$ ground truth data and $DUO$-derived benchmark and contenstant predictions for both $DECONV$ and $DE$ data using t-distributed stochastic neighbor embedding (t-SNE, van der Maaten et al [ref]). Each point represents a single ground truth or predicted sample.

We observe that in $DECONV$ data the samples primarily cluster by pertubagen type, with the exeption of the ground truth $UNI$ data, which appears distinct from the deconvoluted samples (\Fig{tsne_figure} A and B). Separating the samples by algorithm reveals commonalities in the predictions generated by similar algorithms (\Fig{tsne_figure} C). For example, the decision tree regressor (DTR) algorithms have similar 'footprints' in the projection, as do the k-means and Gaussian mixture model (GMM) algorithms. This suggests that in general similar algorithms generate predictions with similar properties.

After the standard transformation to $DE$ data we observe that the t-SNE projection is much more homogenous, indicating that perturbagen type and algorithm-specific affects have been greatly reduced (\Fig{tsne_figure} D). This is reassuring, given that in production mode downstream analysis of this data will be based on $DE$. It also suggests that integrating multiple deconvolution algorithms into an ensemble method might be feasible. 

### Clustering by gene-Level performance

We observed that the global structure of DE data seemed independent of algorithm type in general. We additionally sought to understand whether there were differences between the algorithms at the individual gene level. To assess this, we clustered the algorithms in the space of their gene-level accuracy metrics (Figure XX). We observe that for a large proportion of genes, all algorithms perform about equally well. There are subsets of genes for which accuracy is a bit more algorithm-dependent \color{red}(ex. foo bar).\color{black} These differences suggest complementarity between the algorithms which could potentially be leveraged using an ensemble approach.

\color{red}
It seems that the gene-level performance for the top algorithms is quite similar. There are very few for which one algorithm is notably different than the rest. This does not immediately suggest obvious complementarity between the algorithms, but we can formally test this by aggregating the predictions from all possible algorithm combinations and assessing their accuracy.
\color{black}

## Ensemble approaches

Figure 3. (A) Scatterplot runtime vs accuracy for ensamble (slides p. 163)

Speed vs accuarcy trade-off. Integration one or multiple methods?


## Minors:

- signs of overfitting (compare traing vs testing)


