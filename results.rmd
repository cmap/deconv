
```{r results-setup, include=FALSE}
options(digits=2)
library(data.table)
library(dplyr, warn=FALSE)
library(knitr)
```

```{r datasets, include = FALSE}
speed_acc <- fread("data/holdout_score_time.txt", na.strings="n/a")
soln_desc <- fread("data/solution_descriptions.txt", na.strings="n/a")
speed_acc <- merge(speed_acc, soln_desc, by="handle")
```

```{r main-stats, include = FALSE}
percent <- function (x, digits = 0, brackets = FALSE) {
    pc <- paste0(round(x * 100, digits), "%")
    if (brackets) 
        pc <- paste0("(", pc, ")")
    return(pc)
}
bench.auc <- speed_acc$auc[speed_acc$handle == "benchmark"]
bench.cor <- speed_acc$cor[speed_acc$handle == "benchmark"]
bench.sec <- speed_acc$sec[speed_acc$handle == "benchmark"]
plate_id <- as.numeric(factor(speed_acc$plate))

# avg. improvements
ols.sec <- lm((sec - bench.sec[plate_id])/bench.sec[plate_id] ~ 1, data=speed_acc, subset = handle!="benchmark")
ols.auc <- lm(log(auc / bench.auc[plate_id]) ~ 1, data=speed_acc, subset = handle!="benchmark")
ols.cor <- lm(log(cor / bench.cor[plate_id]) ~ 1, data=speed_acc, subset = handle!="benchmark")
ci <- sapply(list(sec=ols.sec,auc=ols.auc,cor=ols.cor), confint)
speed_acc$sec / bench.sec[plate_id] 
``` 

# Results

The contest attracted 294 participants, who made 820 code submissions with an average of about 18 submissions per participant. The submissions were narrowed down to the top nine scoring submissions in the contest. We illustrate the variety of different analysis methods by outlining the top four below (while \nameref{s3-table} provides brief information on the others).

The winning approach (developed by `gardn999`, a competitor from the United States with a degree in Physics from the University of Kansas) was a random forest algorithm that combined the predictions of 10 different trees. Each tree was trained using 60 different features of the data. These features included the actual measurements for each bead type (binned into 50 different variables), as well as aggregate variables of associations at the plate and experiment level. 

The second best (developed by `Ardavel`, a competitor from Poland with a Master's degree in Computer Science from The Lodz University of Technology) classified the clusters using a Gaussian mixture model fitted with the Expectation-Maximization (EM) algorithm for each gene pair. Next, it improved the predictions using the clustered data to fit a plate-wide distribution of the estimated expression levels and cluster sizes for each gene. The fitted distribution was then used to adjust the peak assignments.

The third best (developed by `mkagenius`, a competitor from India with a bachelor's degree in Computer Science) was a  k-means algorithm with fine adjustments to avoid local minima, minimize the impact of extreme outliers, and improve speed.
  
Finally, the fourth best (developed by `Ramzes2`, a competitor from Ukraine with a bachelor's degree in Computer Science from the Cherkasy National University) was based on a Convolutional Neural Network (CNN). The algorithm first filtered the data to remove extreme outlier measurements. The filtered data were then transformed into a 32-bin histogram for each pair of genes and served as input to a CNN, which consisted of two parts. The first part used the classic U-net architecture [@ronneberger2015u] (a contracting path to capture context and a symmetric expanding path) to provide adequate representation of the data. The output of this network was then used to label bins into one of the two genes for each pair, and to predict the exact value within the bin. This step was achieved using two subnetworks with the same architecture but weights trained separately (using a mean squared error loss function).   

Despite the wide diversity of the approaches described above, all top solutions included ways to automatically remove extreme outlier measurements and to incorporate plate level information to refine the predictions. 
  
## Accuracy and speed

### Correlation 

We tested the accuracy of the solutions by comparing the distribution of the genewise spearman correlation ($\rho$) between the ground-truth gene-expressions (as detected by UNI) and values of the competitors and of the benchmark that were obtained through the deconvolution of DUO data. The Spearman rank correlation was high overall ($\rho$ > `r min(speed_acc$cor)`). However, the empirical distributions of the competitor soutions were right-shifted compared to the k-means benchmark, indicating more accurate predictions in both shRNA and compound experiments (\Fig{accuracy-fig}, A and B). On average, nearly all competitors achieved significant improvements compared to the benchmark (\Fig{accuracy-fig}, C and D), with the random forest approach of the contest winner raising the average correlation by 3 to 4 percentage points (approximately 5% increase over the benchmark).

To understand whether there were differences between the algorithms at the individual gene level, we identified the best performing algorithm (by the correlation metric) for each of the 976 landmark genes (\Fig{accuracy-fig}, E and F). The random forest approach was the best performer for about a third (30%) of the genes. The gaussian mixture model was the best performer for about one fourth (20%). Thus, the top two contest submissions achieved the highest correlation in more than half of the genes. Even so, all but a few algorithms were the best performers for at least 5% of the genes, suggesting some complementarity between the algorithms, which could potentially be leveraged by an ensemble approach, as discussed below.

### Extreme modulations

We further tested the accuracy of the competitors' solutions on the detection of, so called, "extreme modulations."  These are genes that are notably up- or down-regulated by perturbation, and hence exhibit exceedingly high (or low) differential expression (DE) values. We obtained the DE values by using a robust z-score procedure (as described by @subramanian2017next) and evaluated the detection accuracy of each solution by computing the corresponding area under the curve (AUC) using UNI DE data as ground truth. 

All the solutions achieved a good detection accuracy (AUC > `r  min(speed_acc$auc)`). As before, the competitors achieved significant improvements relative to the benchmark in both the shRNA and compound experiments (\Fig{auc}, A, B and C).

We further tested differences in the probability of detection of the extreme modulations for targeted gene knockdowns (KD). These are experiments involving an shRNA targeting one of the 976 landmark genes. Hence, we expected the targeted gene should exhibit a very low DE value (highly negative). For each such experiment, we define a successful KD as one in which the DE value is less than -2 and the gene-wise rank is less than 10, meaning that the targeted gene achieves one of its lowest DE values in the experiment where it was targeted. We computed the KD success frequency for each competitor algorithm as well as the benchmark and UNI data. We used the $UNI$ ground-truth data to estimate the maximum achievable frequency, which in this case was 0.8. We observe that all but 2 of the top 9 contestant algorithms achieve a higher KD success frequency than the benchmark solution (\Fig{auc}, D). These results suggest that the algorithm improvements, as assessed by the accuracy metrics used in the contest, translate to improvements in biologically relevant metrics used in common applications of L1000 data.

Given the variety of methods represented amongst the prize-winning solutions, we sought to assess whether there were notable differences in the predictions generated. Using the holdout dataset, we generated a two-dimensional projection of the UNI ground truth data and DUO-derived benchmark and contestant predictions for both DECONV and DE data using t-distributed stochastic neighbor embedding (t-SNE, @maaten2008visualizing). We observe that in DECONV data the samples primarily cluster by pertubagen type, except for the ground truth UNI data, which appears distinct from the deconvoluted samples (\Fig{tsne_figure}, A and B). Separating the samples by algorithm reveals commonalities in the predictions generated by similar approaches (\Fig{tsne_figure}, C). For example, the decision tree regressor (DTR) algorithms have similar 'footprints' in the projection, as do the k-means and Gaussian mixture model (GMM) algorithms. This suggests that in general similar algorithms generate predictions with similar properties. After the standard transformation to DE data, we observe that the t-SNE projection is much more homogenous, indicating that perturbagen type and algorithm-specific effects have been greatly reduced (\Fig{tsne_figure} D). This is reassuring, given that in production mode the downstream analysis of this data will be based on DE. It also suggests that integrating multiple deconvolution algorithms into an ensemble method might be feasible. 


### Reduction variation across replicate samples

Our data contain several replicates which enabled us to study improvements in inter-replicate variation of gene expression-quantities. This is of crucial practical importance to biologists because a lower variance means higher reproducibility. And one of the issues with the benchmark k-means solution was that it tends to do little to mitigate the discrepancy in variability between the genes measured with high and low bead proportions. To test this hypothesis, we computed the distribution of the sample variance ($s^2$) of the measurements of each gene-perturbagen combination for each approach. The sample size ranged between 4 to 11 replicates of the same perturbagen experiment on a given plate. 
Of particular interest was the difference between distributions for genes in high versus low bead proportions, as we expected those in low bead proportions to have a higher variance. 
Consistent with our expectations, the distribution of sample variances of the benchmark was right-shifted compared to the distribution of the competitors (\Fig{inter-replicate-variance}) and the shift was more visible for genes in low bead proportions. This evidence indicates significant improvements in the inter-replicate variance of the competitors over the benchmark. 

### Computational speed

Speed improvements over the benchmark were substantial (\Fig{speed}). The benchmark took about 4 minutes per plates. In contrast, the fastest algorithm took as little as `r min(speed_acc$sec)` seconds per plate (a 60x speedup compared to the benchmark), and the slowest was well below one minute per plate. We observed no particular trade-off between speed and accuracy. The fastest algorithm (Ardavel), that was based a gaussian mixture model, achieved a good level of accuracy as well, and ranked second overall. On the other hand, the algorithm with the best performance in terms of accuracy (gardn999), which was based on a decision tree regression, also achieved a decent speed performance compared to the benchmark. Thus, at least within the context of the implemented solutions, we found a negligible trade-off between speed and accuracy.


### Ensembles
 

TBA 


